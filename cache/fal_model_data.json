{
  "fal-ai/framepack": {
    "id": "fal-ai/framepack",
    "name": "Framepack",
    "category": "image-to-video",
    "description": "Framepack is an efficient Image-to-video model that autoregressively generates videos.",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "Text prompt for video generation (max 500 characters).",
        "required": true
      },
      "negative_prompt": {
        "type": "string",
        "description": "Negative prompt for video generation.",
        "required": false,
        "default": ""
      },
      "image_url": {
        "type": "string",
        "description": "URL of the image input.",
        "required": true
      },
      "seed": {
        "type": "integer",
        "description": "The seed to use for generating the video.",
        "required": false
      },
      "aspect_ratio": {
        "type": "string",
        "description": "The aspect ratio of the video to generate.",
        "required": false,
        "default": "16:9",
        "enum": [
          "16:9",
          "9:16"
        ]
      },
      "resolution": {
        "type": "string",
        "description": "The resolution of the video to generate. 720p generations cost 1.5x more than 480p generations.",
        "required": false,
        "default": "480p",
        "enum": [
          "720p",
          "480p"
        ]
      },
      "cfg_scale": {
        "type": "number",
        "description": "Classifier-Free Guidance scale for the generation.",
        "required": false,
        "default": 1,
        "min": 0,
        "max": 7
      },
      "guidance_scale": {
        "type": "number",
        "description": "Guidance scale for the generation.",
        "required": false,
        "default": 10,
        "min": 0,
        "max": 32
      },
      "num_frames": {
        "type": "integer",
        "description": "The number of frames to generate.",
        "required": false,
        "default": 180,
        "min": 30,
        "max": 900
      },
      "enable_safety_checker": {
        "type": "boolean",
        "description": "If set to true, the safety checker will be enabled.",
        "required": false,
        "default": false
      }
    },
    "capabilities": [
      "inference",
      "commercial use"
    ],
    "tags": [
      "image to video",
      "motion"
    ],
    "lastUpdated": 1750565421036
  },
  "fal-ai/flux-pro": {
    "id": "fal-ai/flux-pro",
    "name": "FLUX.1 [pro]",
    "category": "text-to-image",
    "description": "FLUX.1 [pro] is a professional-grade model that generates premium quality images from text, optimized for commercial applications and professional creative workflows.",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "The prompt to generate an image from.",
        "required": true
      },
      "num_images": {
        "type": "integer",
        "description": "The number of images to generate.",
        "required": false,
        "default": 1,
        "min": 1,
        "max": 4
      },
      "image_size": {
        "type": "string",
        "description": "The size of the generated image.",
        "required": false,
        "default": "landscape_4_3",
        "enum": [
          "square_hd",
          "square",
          "portrait_4_3",
          "portrait_16_9",
          "landscape_4_3",
          "landscape_16_9"
        ]
      },
      "output_format": {
        "type": "string",
        "description": "The format of the generated image.",
        "required": false,
        "default": "jpeg",
        "enum": [
          "jpeg",
          "png"
        ]
      },
      "sync_mode": {
        "type": "boolean",
        "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response.",
        "required": false,
        "default": false
      },
      "safety_tolerance": {
        "type": "string",
        "description": "The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive.",
        "required": false,
        "default": "2",
        "enum": [
          "1",
          "2",
          "3",
          "4",
          "5",
          "6"
        ]
      },
      "guidance_scale": {
        "type": "number",
        "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.",
        "required": false,
        "default": 3.5,
        "min": 1,
        "max": 20
      },
      "num_inference_steps": {
        "type": "integer",
        "description": "The number of inference steps to perform.",
        "required": false,
        "default": 28,
        "min": 1,
        "max": 50
      },
      "seed": {
        "type": "integer",
        "description": "The same seed and the same prompt given to the same version of the model will output the same image every time.",
        "required": false
      }
    },
    "capabilities": [
      "Inference",
      "Commercial use"
    ],
    "tags": [
      "professional-grade",
      "text-to-image",
      "commercial"
    ],
    "lastUpdated": 1750564793555
  },
  "fal-ai/veo3": {
    "id": "fal-ai/veo3",
    "name": "Veo 3",
    "category": "text-to-video",
    "description": "Veo 3 by Google, the most advanced AI video generation model in the world. With sound on!",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "The text prompt describing the video you want to generate",
        "required": true
      },
      "aspect_ratio": {
        "type": "string",
        "description": "The aspect ratio of the generated video. If it is not set to 16:9, the video will be outpainted with Luma Ray 2 Reframe functionality.",
        "required": false,
        "default": "16:9",
        "enum": [
          "16:9",
          "9:16",
          "1:1"
        ]
      },
      "duration": {
        "type": "string",
        "description": "The duration of the generated video in seconds",
        "required": false,
        "default": "8s",
        "enum": [
          "8s"
        ]
      },
      "generate_audio": {
        "type": "boolean",
        "description": "Whether to generate audio for the video. If false, 33% less credits will be used.",
        "required": false,
        "default": true
      },
      "seed": {
        "type": "integer",
        "description": "A seed to use for the video generation",
        "required": false
      },
      "negative_prompt": {
        "type": "string",
        "description": "A negative prompt to guide the video generation",
        "required": false
      },
      "enhance_prompt": {
        "type": "boolean",
        "description": "Whether to enhance the video generation",
        "required": false,
        "default": true
      }
    },
    "capabilities": [
      "text-to-video",
      "video generation",
      "audio generation"
    ],
    "tags": [
      "inference",
      "commercial"
    ],
    "lastUpdated": 1750564749786
  },
  "fal-ai/minimax": {
    "id": "fal-ai/minimax",
    "name": "Minimax",
    "category": "unknown",
    "description": "Ready-to-use REST inference API, best in class performance, no coldstarts, fair pricing.",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "Text prompt for video generation.",
        "required": true,
        "max": 2000
      },
      "prompt_optimizer": {
        "type": "boolean",
        "description": "Whether to use the model's prompt optimizer.",
        "required": false,
        "default": true
      }
    },
    "capabilities": [
      "text-to-video",
      "image-to-video",
      "text-to-speech",
      "voice-cloning"
    ],
    "tags": [
      "inference",
      "private"
    ],
    "lastUpdated": 1750564756939
  },
  "fal-ai/wan-effects": {
    "id": "fal-ai/wan-effects",
    "name": "Wan Effects",
    "category": "image-to-video",
    "description": "Wan Effects generates high-quality videos with popular effects from images",
    "parameters": {
      "subject": {
        "type": "string",
        "description": "The subject to insert into the predefined prompt template for the selected effect.",
        "required": true
      },
      "image_url": {
        "type": "string",
        "description": "URL of the input image.",
        "required": true
      },
      "effect_type": {
        "type": "string",
        "description": "The type of effect to apply to the video.",
        "required": false,
        "default": "cakeify",
        "enum": [
          "squish",
          "muscle",
          "inflate",
          "crush",
          "rotate",
          "gun-shooting",
          "deflate",
          "cakeify",
          "hulk",
          "baby",
          "bride",
          "classy",
          "puppy",
          "snow-white",
          "disney-princess",
          "mona-lisa",
          "painting",
          "pirate-captain",
          "princess",
          "jungle",
          "samurai",
          "vip",
          "warrior",
          "zen",
          "assassin",
          "timelapse",
          "tsunami",
          "fire",
          "zoom-call",
          "doom-fps",
          "fus-ro-dah",
          "hug-jesus",
          "robot-face-reveal",
          "super-saiyan",
          "jumpscare",
          "laughing",
          "cartoon-jaw-drop",
          "crying",
          "kissing",
          "angry-face",
          "selfie-younger-self",
          "animeify",
          "blast"
        ]
      },
      "num_frames": {
        "type": "integer",
        "description": "Number of frames to generate.",
        "required": false,
        "default": 81,
        "min": 81,
        "max": 100
      },
      "frames_per_second": {
        "type": "integer",
        "description": "Frames per second of the generated video.",
        "required": false,
        "default": 16,
        "min": 5,
        "max": 24
      },
      "seed": {
        "type": "integer",
        "description": "Random seed for reproducibility. If None, a random seed is chosen.",
        "required": false
      },
      "aspect_ratio": {
        "type": "string",
        "description": "Aspect ratio of the output video.",
        "required": false,
        "default": "16:9",
        "enum": [
          "16:9",
          "9:16",
          "1:1"
        ]
      },
      "num_inference_steps": {
        "type": "integer",
        "description": "Number of inference steps for sampling. Higher values give better quality but take longer.",
        "required": false,
        "default": 30,
        "min": 2,
        "max": 40
      },
      "lora_scale": {
        "type": "number",
        "description": "The scale of the LoRA weight. Used to adjust effect intensity.",
        "required": false,
        "default": 1,
        "min": 0.1,
        "max": 2
      },
      "turbo_mode": {
        "type": "boolean",
        "description": "Whether to use turbo mode. If True, the video will be generated faster but with lower quality.",
        "required": false,
        "default": false
      }
    },
    "capabilities": [
      "image-to-video",
      "motion"
    ],
    "tags": [
      "motion",
      "effects"
    ],
    "lastUpdated": 1750564816753
  },
  "fal-ai/veo2": {
    "id": "fal-ai/veo2",
    "name": "Veo 2",
    "category": "text-to-video",
    "description": "Veo 2 creates videos with realistic motion and high quality output. Explore different styles and find your own with extensive camera controls.",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "The text prompt describing the video you want to generate",
        "required": true,
        "default": null,
        "min": null,
        "max": null,
        "enum": null
      },
      "aspect_ratio": {
        "type": "string",
        "description": "The aspect ratio of the generated video",
        "required": false,
        "default": "16:9",
        "min": null,
        "max": null,
        "enum": [
          "16:9",
          "9:16"
        ]
      },
      "duration": {
        "type": "string",
        "description": "The duration of the generated video in seconds",
        "required": false,
        "default": "5s",
        "min": null,
        "max": null,
        "enum": [
          "5s",
          "6s",
          "7s",
          "8s"
        ]
      },
      "negative_prompt": {
        "type": "string",
        "description": "A negative prompt to guide the video generation",
        "required": false,
        "default": null,
        "min": null,
        "max": null,
        "enum": null
      },
      "enhance_prompt": {
        "type": "boolean",
        "description": "Whether to enhance the video generation",
        "required": false,
        "default": true,
        "min": null,
        "max": null,
        "enum": null
      },
      "seed": {
        "type": "integer",
        "description": "A seed to use for the video generation",
        "required": false,
        "default": null,
        "min": null,
        "max": null,
        "enum": null
      }
    },
    "capabilities": [
      "text-to-video",
      "high-quality output",
      "extensive camera controls"
    ],
    "tags": [
      "motion",
      "transformation"
    ],
    "lastUpdated": 1750564818797
  },
  "fal-ai/flux-lora-fast-training": {
    "id": "fal-ai/flux-lora-fast-training",
    "name": "Train Flux LoRA",
    "category": "training",
    "description": "Train styles, people and other subjects at blazing speeds.",
    "parameters": {
      "images_data_url": {
        "type": "string",
        "description": "URL to zip archive with images. Try to use at least 4 images in general the more the better. In addition to images the archive can contain text files with captions. Each text file should have the same name as the image file it corresponds to.",
        "required": true
      },
      "trigger_word": {
        "type": "string",
        "description": "Trigger word to be used in the captions. If None, a trigger word will not be used. If no captions are provide the trigger_word will be used instead of captions. If captions are the trigger word will not be used.",
        "required": false
      },
      "create_masks": {
        "type": "boolean",
        "description": "If True segmentation masks will be used in the weight the training loss. For people a face mask is used if possible.",
        "required": false,
        "default": true
      },
      "steps": {
        "type": "number",
        "description": "Number of steps to train the LoRA on.",
        "required": false,
        "default": 1000,
        "min": 1,
        "max": 10000
      },
      "is_style": {
        "type": "boolean",
        "description": "If True, the training will be for a style. This will deactivate segmentation, captioning and will use trigger word instead. Use the trigger word to specify the style.",
        "required": false,
        "default": false
      },
      "is_input_format_already_preprocessed": {
        "type": "boolean",
        "description": "Specifies whether the input data is already in a processed format. When set to False (default), the system expects raw input where image files and their corresponding caption files share the same name (e.g., 'photo.jpg' and 'photo.txt'). Set to True if your data is already in a preprocessed format.",
        "required": false,
        "default": false
      },
      "data_archive_format": {
        "type": "string",
        "description": "The format of the archive. If not specified, the format will be inferred from the URL.",
        "required": false
      }
    },
    "capabilities": [
      "training",
      "fine-tuning",
      "LoRA"
    ],
    "tags": [
      "lora",
      "personalization"
    ],
    "lastUpdated": 1750564835226
  },
  "fal-ai/hidream-i1-fast": {
    "id": "fal-ai/hidream-i1-fast",
    "name": "Hidream I1 Fast",
    "category": "text-to-image",
    "description": "HiDream-I1 fast is a new open-source image generative foundation model with 17B parameters that achieves state-of-the-art image generation quality within 16 steps.",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "The prompt to generate an image from.",
        "required": true,
        "default": ""
      },
      "negative_prompt": {
        "type": "string",
        "description": "The negative prompt to use. Use it to address details that you don't want in the image.",
        "required": false,
        "default": ""
      },
      "image_size": {
        "type": "object|string",
        "description": "The size of the generated image.",
        "required": false,
        "default": {
          "height": 1024,
          "width": 1024
        },
        "enum": [
          "square_hd",
          "square",
          "portrait_4_3",
          "portrait_16_9",
          "landscape_4_3",
          "landscape_16_9"
        ]
      },
      "num_inference_steps": {
        "type": "integer",
        "description": "The number of inference steps to perform.",
        "required": false,
        "default": 16,
        "min": 1,
        "max": 50
      },
      "seed": {
        "type": "integer",
        "description": "The same seed and the same prompt given to the same version of the model will output the same image every time.",
        "required": false
      },
      "sync_mode": {
        "type": "boolean",
        "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response.",
        "required": false,
        "default": false
      },
      "num_images": {
        "type": "integer",
        "description": "The number of images to generate.",
        "required": false,
        "default": 1,
        "min": 1,
        "max": 4
      },
      "enable_safety_checker": {
        "type": "boolean",
        "description": "If set to true, the safety checker will be enabled.",
        "required": false,
        "default": true
      },
      "output_format": {
        "type": "string",
        "description": "The format of the generated image.",
        "required": false,
        "default": "jpeg",
        "enum": [
          "jpeg",
          "png"
        ]
      }
    },
    "capabilities": [
      "Inference",
      "Commercial use",
      "Streaming"
    ],
    "tags": [
      ""
    ],
    "lastUpdated": 1750564876059
  },
  "fal-ai/mmaudio-v2": {
    "id": "fal-ai/mmaudio-v2",
    "name": "MMAudio V2",
    "category": "video-to-video",
    "description": "MMAudio generates synchronized audio given video and/or text inputs. It can be combined with video models to get videos with audio.",
    "parameters": {
      "video_url": {
        "type": "string",
        "description": "The URL of the video to generate the audio for.",
        "required": true
      },
      "prompt": {
        "type": "string",
        "description": "The prompt to generate the audio for.",
        "required": true
      },
      "num_steps": {
        "type": "number",
        "description": "The number of steps to generate the audio for.",
        "required": false,
        "default": 25,
        "min": 4,
        "max": 50
      },
      "duration": {
        "type": "number",
        "description": "The duration of the audio to generate.",
        "required": false,
        "default": 8,
        "min": 1,
        "max": 30
      },
      "cfg_strength": {
        "type": "number",
        "description": "The strength of Classifier Free Guidance.",
        "required": false,
        "default": 4.5,
        "min": 0,
        "max": 20
      },
      "seed": {
        "type": "number",
        "description": "The seed for the random number generator.",
        "required": false,
        "min": 0,
        "max": 65535
      },
      "mask_away_clip": {
        "type": "boolean",
        "description": "Whether to mask away the clip.",
        "required": false,
        "default": false
      },
      "negative_prompt": {
        "type": "string",
        "description": "The negative prompt to generate the audio for.",
        "required": false,
        "default": ""
      }
    },
    "capabilities": [
      "generate synchronized audio",
      "combine with video models"
    ],
    "tags": [
      "ai video",
      "fast"
    ],
    "lastUpdated": 1750564882603
  },
  "fal-ai/hidream-i1-dev": {
    "id": "fal-ai/hidream-i1-dev",
    "name": "Hidream I1 Dev",
    "category": "text-to-image",
    "description": "HiDream-I1 dev is a new open-source image generative foundation model with 17B parameters that achieves state-of-the-art image generation quality within seconds.",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "The prompt to generate an image from.",
        "required": true,
        "default": "a cat holding a skateboard which has 'fal' written on it in red spray paint"
      },
      "negative_prompt": {
        "type": "string",
        "description": "The negative prompt to use. Use it to address details that you don't want in the image.",
        "required": false,
        "default": ""
      },
      "image_size": {
        "type": "object",
        "description": "The size of the generated image.",
        "required": false,
        "default": {
          "height": 1024,
          "width": 1024
        }
      },
      "num_inference_steps": {
        "type": "integer",
        "description": "The number of inference steps to perform.",
        "required": false,
        "default": 28,
        "min": 1,
        "max": 50
      },
      "seed": {
        "type": "integer",
        "description": "The same seed and the same prompt given to the same version of the model will output the same image every time.",
        "required": false
      },
      "sync_mode": {
        "type": "boolean",
        "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response.",
        "required": false,
        "default": false
      },
      "num_images": {
        "type": "integer",
        "description": "The number of images to generate.",
        "required": false,
        "default": 1,
        "min": 1,
        "max": 4
      },
      "enable_safety_checker": {
        "type": "boolean",
        "description": "If set to true, the safety checker will be enabled.",
        "required": false,
        "default": true
      },
      "output_format": {
        "type": "string",
        "description": "The format of the generated image.",
        "required": false,
        "default": "jpeg",
        "enum": [
          "jpeg",
          "png"
        ]
      }
    },
    "capabilities": [
      "Inference",
      "Commercial use",
      "Streaming"
    ],
    "tags": [],
    "lastUpdated": 1750564884737
  },
  "fal-ai/hidream-i1-full": {
    "id": "fal-ai/hidream-i1-full",
    "name": "Hidream I1 Full",
    "category": "text-to-image",
    "description": "HiDream-I1 full is a new open-source image generative foundation model with 17B parameters that achieves state-of-the-art image generation quality within seconds.",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "The prompt to generate an image from.",
        "required": true
      },
      "negative_prompt": {
        "type": "string",
        "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution).",
        "required": false,
        "default": ""
      },
      "image_size": {
        "type": "object",
        "description": "The size of the generated image.",
        "required": false,
        "default": {
          "height": 1024,
          "width": 1024
        }
      },
      "num_inference_steps": {
        "type": "number",
        "description": "The number of inference steps to perform.",
        "required": false,
        "default": 50,
        "min": 1,
        "max": 50
      },
      "seed": {
        "type": "number",
        "description": "The same seed and the same prompt given to the same version of the model will output the same image every time.",
        "required": false
      },
      "guidance_scale": {
        "type": "number",
        "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.",
        "required": false,
        "default": 5,
        "min": 0,
        "max": 20
      },
      "sync_mode": {
        "type": "boolean",
        "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.",
        "required": false,
        "default": false
      },
      "num_images": {
        "type": "number",
        "description": "The number of images to generate.",
        "required": false,
        "default": 1,
        "min": 1,
        "max": 4
      },
      "enable_safety_checker": {
        "type": "boolean",
        "description": "If set to true, the safety checker will be enabled.",
        "required": false,
        "default": true
      },
      "output_format": {
        "type": "string",
        "description": "The format of the generated image.",
        "required": false,
        "default": "jpeg",
        "enum": [
          "jpeg",
          "png"
        ]
      },
      "loras": {
        "type": "array",
        "description": "A list of LoRAs to apply to the model. Each LoRA specifies its path, scale, and optional weight name.",
        "required": false,
        "default": []
      }
    },
    "capabilities": [
      "Inference",
      "Commercial use",
      "Streaming"
    ],
    "tags": [],
    "lastUpdated": 1750564885233
  },
  "fal-ai/flux": {
    "id": "fal-ai/flux/schnell",
    "name": "FLUX.1 [schnell]",
    "category": "text-to-image",
    "description": "FLUX.1 [schnell] is a 12 billion parameter flow transformer that generates high-quality images from text in 1 to 4 steps, suitable for personal and commercial use.",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "The prompt to generate an image from.",
        "required": true
      },
      "image_size": {
        "type": "string",
        "description": "The size of the generated image.",
        "required": false,
        "default": "landscape_4_3",
        "enum": [
          "square_hd",
          "square",
          "portrait_4_3",
          "portrait_16_9",
          "landscape_4_3",
          "landscape_16_9"
        ]
      },
      "num_inference_steps": {
        "type": "number",
        "description": "The number of inference steps to perform.",
        "required": false,
        "default": 4,
        "min": 1,
        "max": 12
      },
      "seed": {
        "type": "number",
        "description": "The same seed and the same prompt given to the same version of the model will output the same image every time.",
        "required": false
      },
      "sync_mode": {
        "type": "boolean",
        "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response.",
        "required": false,
        "default": false
      },
      "num_images": {
        "type": "number",
        "description": "The number of images to generate.",
        "required": false,
        "default": 1,
        "min": 1,
        "max": 4
      },
      "enable_safety_checker": {
        "type": "boolean",
        "description": "If set to true, the safety checker will be enabled.",
        "required": false,
        "default": true
      }
    },
    "capabilities": [
      "text-to-image",
      "high-quality image generation",
      "commercial use"
    ],
    "tags": [],
    "lastUpdated": 1750564896038
  },
  "fal-ai/flux-lora-portrait-trainer": {
    "id": "fal-ai/flux-lora-portrait-trainer",
    "name": "Train Flux LoRAs For Portraits",
    "category": "training",
    "description": "FLUX LoRA training optimized for portrait generation, with bright highlights, excellent prompt following and highly detailed results.",
    "parameters": {
      "images_data_url": {
        "type": "string",
        "description": "URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better. In addition to images the archive can contain text files with captions. Each text file should have the same name as the image file it corresponds to. The captions can include a special string `[trigger]`. If a trigger_word is specified, it will replace `[trigger]` in the captions.",
        "required": true
      },
      "trigger_phrase": {
        "type": "string",
        "description": "Trigger phrase to be used in the captions. If None, a trigger word will not be used. If no captions are provide the trigger_work will be used instead of captions. If captions are provided, the trigger word will replace the `[trigger]` string in the captions.",
        "required": false
      },
      "learning_rate": {
        "type": "number",
        "description": "Learning rate to use for training. Default value: `0.00009`",
        "required": false,
        "default": 0.00009,
        "min": 0.000001,
        "max": 0.001
      },
      "steps": {
        "type": "integer",
        "description": "Number of steps to train the LoRA on. Default value: `2500`",
        "required": false,
        "default": 2500,
        "min": 1,
        "max": 10000
      },
      "multiresolution_training": {
        "type": "boolean",
        "description": "If True, multiresolution training will be used. Default value: `true`",
        "required": false,
        "default": true
      },
      "subject_crop": {
        "type": "boolean",
        "description": "If True, the subject will be cropped from the image. Default value: `true`",
        "required": false,
        "default": true
      },
      "data_archive_format": {
        "type": "string",
        "description": "The format of the archive. If not specified, the format will be inferred from the URL.",
        "required": false
      },
      "resume_from_checkpoint": {
        "type": "string",
        "description": "URL to a checkpoint to resume training from. Default value: `\"\"`",
        "required": false,
        "default": ""
      },
      "create_masks": {
        "type": "boolean",
        "description": "If True, masks will be created for the subject.",
        "required": false,
        "default": false
      }
    },
    "capabilities": [
      "portrait generation",
      "LoRA training"
    ],
    "tags": [
      "lora",
      "personalization"
    ],
    "lastUpdated": 1750564913894
  },
  "fal-ai/flux-lora": {
    "id": "fal-ai/flux-lora",
    "name": "FLUX.1 [dev] with LoRAs",
    "category": "text-to-image",
    "description": "Super fast endpoint for the FLUX.1 [dev] model with LoRA support, enabling rapid and high-quality image generation using pre-trained LoRA adaptations for personalization, specific styles, brand identities, and product-specific outputs.",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "The prompt to generate an image from.",
        "required": true,
        "default": ""
      },
      "num_images": {
        "type": "integer",
        "description": "The number of images to generate. This is always set to 1 for streaming output.",
        "required": false,
        "default": 1,
        "min": 1,
        "max": 4
      },
      "image_size": {
        "type": "string",
        "description": "The size of the generated image.",
        "required": false,
        "default": "landscape_4_3",
        "enum": [
          "square_hd",
          "square",
          "portrait_4_3",
          "portrait_16_9",
          "landscape_4_3",
          "landscape_16_9"
        ]
      },
      "output_format": {
        "type": "string",
        "description": "The format of the generated image.",
        "required": false,
        "default": "jpeg",
        "enum": [
          "jpeg",
          "png"
        ]
      },
      "loras": {
        "type": "array",
        "description": "The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image.",
        "required": false,
        "default": []
      },
      "sync_mode": {
        "type": "boolean",
        "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.",
        "required": false,
        "default": false
      },
      "guidance_scale": {
        "type": "number",
        "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.",
        "required": false,
        "default": 3.5,
        "min": 0,
        "max": 35
      },
      "num_inference_staps": {
        "type": "integer",
        "description": "The number of inference steps to perform.",
        "required": false,
        "default": 28,
        "min": 1,
        "max": 50
      },
      "seed": {
        "type": "integer",
        "description": "The same seed and the same prompt given to the same version of the model will output the same image every time.",
        "required": false
      },
      "enable_safety_checker": {
        "type": "boolean",
        "description": "If set to true, the safety checker will be enabled.",
        "required": false,
        "default": true
      }
    },
    "capabilities": [
      "image-generation",
      "lora-support",
      "style-transfer"
    ],
    "tags": [
      "lora",
      "personalization"
    ],
    "lastUpdated": 1750564931385
  },
  "fal-ai/stable-diffusion-v35-large": {
    "id": "fal-ai/stable-diffusion-v35-large",
    "name": "Stable Diffusion 3.5 Large",
    "category": "text-to-image",
    "description": "Stable Diffusion 3.5 Large is a Multimodal Diffusion Transformer (MMDiT) text-to-image model that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency.",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "The prompt to generate an image from.",
        "required": true
      },
      "negative_prompt": {
        "type": "string",
        "description": "The negative prompt to use. Use it to address details that you don't want in the image.",
        "required": false,
        "default": ""
      },
      "num_inference_steps": {
        "type": "number",
        "description": "The number of inference steps to perform.",
        "required": false,
        "default": 28,
        "min": 1,
        "max": 50
      },
      "guidance_scale": {
        "type": "number",
        "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt.",
        "required": false,
        "default": 3.5,
        "min": 0,
        "max": 20
      },
      "num_images": {
        "type": "number",
        "description": "The number of images to generate.",
        "required": false,
        "default": 1,
        "min": 1,
        "max": 4
      },
      "enable_safety_checker": {
        "type": "boolean",
        "description": "If set to true, the safety checker will be enabled.",
        "required": false,
        "default": true
      },
      "output_format": {
        "type": "string",
        "description": "The format of the generated image.",
        "required": false,
        "default": "jpeg",
        "enum": [
          "jpeg",
          "png"
        ]
      }
    },
    "capabilities": [
      "text-to-image",
      "image generation",
      "typography",
      "complex prompt understanding"
    ],
    "tags": [
      "diffusion",
      "typography",
      "style"
    ],
    "lastUpdated": 1750564945622
  },
  "fal-ai/video-understanding": {
    "id": "fal-ai/video-understanding",
    "name": "Video Understanding",
    "category": "vision",
    "description": "A video understanding model to analyze video content and answer questions about what's happening in the video based on user prompts.",
    "parameters": {
      "video_url": {
        "type": "string",
        "description": "URL of the video to analyze",
        "required": true
      },
      "prompt": {
        "type": "string",
        "description": "The question or prompt about the video content",
        "required": true
      },
      "detailed_analysis": {
        "type": "boolean",
        "description": "Whether to request a more detailed analysis of the video",
        "required": false,
        "default": false
      }
    },
    "capabilities": [
      "video analysis",
      "question answering"
    ],
    "tags": [
      "utility",
      "vision"
    ],
    "lastUpdated": 1750564994519
  },
  "fal-ai/clarity-upscaler": {
    "id": "fal-ai/clarity-upscaler",
    "name": "Clarity Upscaler",
    "category": "image-to-image",
    "description": "Clarity upscaler for upscaling images with high very fidelity.",
    "parameters": {
      "image_url": {
        "type": "string",
        "description": "The URL of the image to upscale.",
        "required": true
      },
      "prompt": {
        "type": "string",
        "description": "The prompt to use for generating the image. Be as descriptive as possible for best results.",
        "required": false,
        "default": "masterpiece, best quality, highres"
      },
      "upscale_factor": {
        "type": "number",
        "description": "The upscale factor.",
        "required": false,
        "default": 2,
        "min": 1,
        "max": 4
      },
      "negative_prompt": {
        "type": "string",
        "description": "The negative prompt to use. Use it to address details that you don't want in the image.",
        "required": false,
        "default": "(worst quality, low quality, normal quality:2)"
      },
      "creativity": {
        "type": "number",
        "description": "The creativity of the model. The higher the creativity, the more the model will deviate from the prompt.",
        "required": false,
        "default": 0.35,
        "min": 0,
        "max": 1
      },
      "resemblance": {
        "type": "number",
        "description": "The resemblance of the upscaled image to the original image. The higher the resemblance, the more the model will try to keep the original image.",
        "required": false,
        "default": 0.6,
        "min": 0,
        "max": 1
      },
      "guidance_scale": {
        "type": "number",
        "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.",
        "required": false,
        "default": 4,
        "min": 0,
        "max": 20
      },
      "num_inference_steps": {
        "type": "integer",
        "description": "The number of inference steps to perform.",
        "required": false,
        "default": 18,
        "min": 4,
        "max": 50
      },
      "seed": {
        "type": "integer",
        "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time.",
        "required": false
      },
      "enable_safety_checker": {
        "type": "boolean",
        "description": "If set to false, the safety checker will be disabled.",
        "required": false,
        "default": true
      }
    },
    "capabilities": [
      "inference",
      "commercial use"
    ],
    "tags": [
      "upscaling"
    ],
    "lastUpdated": 1750564998105
  },
  "fal-ai/aura-sr": {
    "id": "fal-ai/aura-sr",
    "name": "AuraSR",
    "category": "image-to-image",
    "description": "Upscale your images with AuraSR.",
    "parameters": {
      "image_url": {
        "type": "string",
        "description": "URL of the image to upscale.",
        "required": true,
        "default": null
      },
      "upscaling_factor": {
        "type": "number",
        "description": "Upscaling factor. More coming soon.",
        "required": false,
        "default": 4,
        "enum": [
          4
        ]
      },
      "overlapping_tiles": {
        "type": "boolean",
        "description": "Whether to use overlapping tiles for upscaling. Setting this to true helps remove seams but doubles the inference time.",
        "required": false,
        "default": false
      },
      "checkpoint": {
        "type": "string",
        "description": "Checkpoint to use for upscaling. More coming soon.",
        "required": false,
        "default": "v1",
        "enum": [
          "v1",
          "v2"
        ]
      }
    },
    "capabilities": [
      "upscaling",
      "high-res"
    ],
    "tags": [
      "upscaling",
      "high-res"
    ],
    "lastUpdated": 1750565020134
  },
  "fal-ai/aura-flow": {
    "id": "fal-ai/aura-flow",
    "name": "AuraFlow",
    "category": "text-to-image",
    "description": "AuraFlow v0.3 is an open-source flow-based text-to-image generation model that achieves state-of-the-art results on GenEval. The model is currently in beta.",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "The prompt to generate images from",
        "required": true,
        "default": null
      },
      "num_images": {
        "type": "number",
        "description": "The number of images to generate",
        "required": false,
        "default": 1,
        "min": 1,
        "max": 2
      },
      "guidance_scale": {
        "type": "number",
        "description": "Classifier free guidance scale",
        "required": false,
        "default": 3.5,
        "min": 0,
        "max": 20
      },
      "seed": {
        "type": "number",
        "description": "The seed to use for generating images",
        "required": false,
        "default": null
      },
      "expand_prompt": {
        "type": "boolean",
        "description": "Whether to perform prompt expansion (recommended)",
        "required": false,
        "default": true
      },
      "num_inference_steps": {
        "type": "number",
        "description": "The number of inference steps to take",
        "required": false,
        "default": 50,
        "min": 20,
        "max": 50
      }
    },
    "capabilities": [
      "inference",
      "commercial use"
    ],
    "tags": [
      "typography",
      "style"
    ],
    "lastUpdated": 1750565023086
  },
  "fal-ai/chain-of-zoom": {
    "id": "fal-ai/chain-of-zoom",
    "name": "Chain Of Zoom",
    "category": "image-to-image",
    "description": "Extreme Super-Resolution via Scale Autoregression and Preference Alignment",
    "parameters": {
      "image_url": {
        "type": "string",
        "description": "Input image to zoom into",
        "required": true,
        "default": null
      },
      "scale": {
        "type": "number",
        "description": "Zoom scale in powers of 2",
        "required": false,
        "default": 5,
        "min": 1,
        "max": 8
      },
      "center_x": {
        "type": "number",
        "description": "X coordinate of zoom center (0-1)",
        "required": false,
        "default": 0.5,
        "min": 0,
        "max": 1
      },
      "center_y": {
        "type": "number",
        "description": "Y coordinate of zoom center (0-1)",
        "required": false,
        "default": 0.5,
        "min": 0,
        "max": 1
      },
      "user_prompt": {
        "type": "string",
        "description": "Additional prompt text to guide the zoom enhancement",
        "required": false,
        "default": ""
      },
      "sync_mode": {
        "type": "boolean",
        "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response.",
        "required": false,
        "default": false
      }
    },
    "capabilities": [
      "inference"
    ],
    "tags": [
      "super-resolution",
      "image-enhancement"
    ],
    "lastUpdated": 1750565057229
  },
  "fal-ai/turbo-flux-trainer": {
    "id": "fal-ai/turbo-flux-trainer",
    "name": "Turbo Flux Trainer",
    "category": "training",
    "description": "A blazing fast FLUX dev LoRA trainer for subjects and styles.",
    "parameters": {
      "images_data_url": {
        "type": "string",
        "description": "URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.",
        "required": true
      },
      "trigger_phrase": {
        "type": "string",
        "description": "Trigger phrase to be used in the captions. If None, a trigger word will not be used. If no captions are provided, the trigger word will be used instead of captions. If captions are provided, the trigger word will replace the `[trigger]` string in the captions.",
        "required": false,
        "default": "ohwx"
      },
      "steps": {
        "type": "number",
        "description": "Number of steps to train the LoRA on.",
        "required": false,
        "default": 1000,
        "min": 1,
        "max": 10000
      },
      "learning_rate": {
        "type": "number",
        "description": "Learning rate for the training.",
        "required": false,
        "default": 0.00115,
        "min": 1e-7,
        "max": 0.01
      },
      "training_style": {
        "type": "string",
        "description": "Training style to use.",
        "required": false,
        "default": "subject",
        "enum": [
          "subject",
          "style"
        ]
      },
      "face_crop": {
        "type": "boolean",
        "description": "Whether to try to detect the face and crop the images to the face.",
        "required": false,
        "default": true
      }
    },
    "capabilities": [
      "training",
      "commercial use"
    ],
    "tags": [],
    "lastUpdated": 1750565062200
  },
  "fal-ai/flux-lora-general-training": {
    "id": "fal-ai/flux-lora-general-training",
    "name": "Train Flux LoRA (legacy)",
    "category": "training",
    "description": "General Purpose LORA Training for Flux.",
    "parameters": {
      "images_data_url": {
        "type": "string",
        "description": "URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better. In addition to images the archive can contain text files with captions. Each text file should have the same name as the image file it corresponds to. The captions can include a special string `[trigger]`. If a trigger_word is specified, it will replace `[trigger]` in the captions.",
        "required": true
      },
      "trigger_word": {
        "type": "string",
        "description": "Trigger word to be used in the captions. If None, a trigger word will not be used. If no captions are provided the trigger_word will be used instead of captions. If captions are provided, the trigger word will replace the `[trigger]` string in the captions.",
        "required": false
      },
      "steps": {
        "type": "number",
        "description": "Total number of training steps to perform. Default is 1000.",
        "required": false,
        "default": 1000,
        "min": 1,
        "max": 4000
      },
      "rank": {
        "type": "number",
        "description": "Rank of the model. Default is 16.",
        "required": false,
        "default": 16,
        "min": 1,
        "max": 64
      },
      "learning_rate": {
        "type": "number",
        "description": "Initial learning rate for the unet. Default is 4e-4.",
        "required": false,
        "default": 0.0004
      },
      "caption_dropout_rate": {
        "type": "number",
        "description": "Dropout rate for captions. Default is 0.05.",
        "required": false,
        "default": 0.05,
        "min": 0,
        "max": 1
      },
      "high_resolution_mode": {
        "type": "boolean",
        "description": "If true, will only train with the 1024 resolution bucket. Default is False. If True increases the price by 20% (price multiplied by 1.2).",
        "required": false,
        "default": false
      },
      "experimental_optimizers": {
        "type": "string",
        "description": "Experimental. Could change in the future. Default is 'adamw8bit'.",
        "required": false,
        "default": "adamw8bit",
        "enum": [
          "adamw8bit",
          "prodigy",
          "adafactor"
        ]
      },
      "experimental_multi_checkpoints_count": {
        "type": "number",
        "description": "Experimental. Could change in the future. Number of checkpoints to save. Default is 1. Checkpoints are only saved if the interval is set.",
        "required": false,
        "default": 1,
        "min": 1,
        "max": 4
      },
      "experimental_multi_checkpoints_interval": {
        "type": "number",
        "description": "Experimental. Could change in the future. Interval between saving checkpoints. Default is None. If not None must be greater than 250.",
        "required": false
      }
    },
    "capabilities": [
      "training",
      "commercial use"
    ],
    "tags": [
      "lora",
      "training"
    ],
    "lastUpdated": 1750565063757
  },
  "fal-ai/flux-realism": {
    "id": "fal-ai/flux-realism",
    "name": "Flux Realism",
    "category": "private",
    "description": "flux realism endpoint",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "The prompt to generate an image from.",
        "required": true
      },
      "image_size": {
        "type": "string",
        "description": "The size of the generated image.",
        "required": false,
        "default": "landscape_4_3",
        "enum": [
          "square_hd",
          "square",
          "portrait_4_3",
          "portrait_16_9",
          "landscape_4_3",
          "landscape_16_9"
        ]
      },
      "num_inference_steps": {
        "type": "integer",
        "description": "The number of inference steps to perform.",
        "required": false,
        "default": 28,
        "min": 1,
        "max": 50
      },
      "guidance_scale": {
        "type": "number",
        "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.",
        "required": false,
        "default": 3.5,
        "min": 0,
        "max": 20
      },
      "num_images": {
        "type": "integer",
        "description": "The number of images to generate.",
        "required": false,
        "default": 1,
        "min": 1,
        "max": 1
      },
      "enable_safety_checker": {
        "type": "boolean",
        "description": "If set to true, the safety checker will be enabled.",
        "required": false,
        "default": true
      },
      "strength": {
        "type": "number",
        "description": "The strength of the model.",
        "required": false,
        "default": 1,
        "min": 0.1,
        "max": 1
      },
      "output_format": {
        "type": "string",
        "description": "The output image format.",
        "required": false,
        "default": "jpeg",
        "enum": [
          "jpeg",
          "png"
        ]
      }
    },
    "capabilities": [
      "image generation"
    ],
    "tags": [
      "inference",
      "private"
    ],
    "lastUpdated": 1750565068897
  },
  "fal-ai/recraft-v3": {
    "id": "fal-ai/recraft/v3/text-to-image",
    "name": "Recraft V3",
    "category": "text-to-image",
    "description": "Recraft V3 is a text-to-image model with the ability to generate long texts, vector art, images in brand style, and much more. As of today, it is SOTA in image generation, proven by Hugging Face's industry-leading Text-to-Image Benchmark by Artificial Analysis.",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "The text prompt for generating the image.",
        "required": true,
        "default": "",
        "min": 1,
        "max": 1000
      },
      "image_size": {
        "type": "string",
        "description": "The size of the generated image.",
        "required": false,
        "default": "square_hd",
        "enum": [
          "square_hd",
          "square",
          "portrait_4_3",
          "portrait_16_9",
          "landscape_4_3",
          "landscape_16_9"
        ]
      },
      "style": {
        "type": "string",
        "description": "The style of the generated images. Vector images cost 2X as much.",
        "required": false,
        "default": "realistic_image",
        "enum": [
          "any",
          "realistic_image",
          "digital_illustration",
          "vector_illustration",
          "realistic_image/b_and_w",
          "realistic_image/enterprise",
          "realistic_image/hard_flash",
          "realistic_image/hdr",
          "realistic_image/motion_blur",
          "realistic_image/natural_light",
          "realistic_image/studio_portrait",
          "digital_illustration/2d_art_poster",
          "digital_illustration/2d_art_poster_2",
          "digital_illustration/3d",
          "digital_illustration/80s",
          "digital_illustration/engraving_color",
          "digital_illustration/glow",
          "digital_illustration/grain",
          "digital_illustration/hand_drawn",
          "digital_illustration/hand_drawn_outline",
          "digital_illustration/handmade_3d",
          "digital_illustration/infantile_sketch",
          "digital_illustration/kawaii",
          "digital_illustration/pixel_art",
          "digital_illustration/psychedelic",
          "digital_illustration/seamless",
          "digital_illustration/voxel",
          "digital_illustration/watercolor",
          "vector_illustration/cartoon",
          "vector_illustration/doodle_line_art",
          "vector_illustration/engraving",
          "vector_illustration/flat_2",
          "vector_illustration/kawaii",
          "vector_illustration/line_art",
          "vector_illustration/line_circuit",
          "vector_illustration/linocut",
          "vector_illustration/seamless",
          "icon/broken_line",
          "icon/colored_outline",
          "icon/colored_shapes",
          "icon/colored_shapes_gradient",
          "icon/doodle_fill",
          "icon/doodle_offset_fill",
          "icon/offset_fill",
          "icon/outline",
          "icon/outline_gradient",
          "icon/uneven_fill"
        ]
      },
      "colors": {
        "type": "array",
        "description": "An array of preferable colors.",
        "required": false,
        "default": []
      },
      "style_id": {
        "type": "string",
        "description": "The ID of the custom style reference (optional).",
        "required": false,
        "default": ""
      },
      "enable_safety_checker": {
        "type": "boolean",
        "description": "If set to true, the safety checker will be enabled.",
        "required": false,
        "default": false
      }
    },
    "capabilities": [
      "text-to-image",
      "vector art",
      "brand style images"
    ],
    "tags": [
      "vector",
      "typography",
      "style"
    ],
    "lastUpdated": 1750565075439
  },
  "fal-ai/dwpose": {
    "id": "fal-ai/dwpose",
    "name": "DWPose Pose Prediction",
    "category": "image-to-image",
    "description": "Predict poses from images.",
    "parameters": {
      "image_url": {
        "type": "string",
        "description": "URL of the image to be processed",
        "required": true,
        "default": null
      },
      "draw_mode": {
        "type": "string",
        "description": "Mode of drawing the pose on the image. Options are: 'full-pose', 'body-pose', 'face-pose', 'hand-pose', 'face-hand-mask', 'face-mask', 'hand-mask'.",
        "required": false,
        "default": "body-pose",
        "enum": [
          "full-pose",
          "body-pose",
          "face-pose",
          "hand-pose",
          "face-hand-mask",
          "face-mask",
          "hand-mask"
        ]
      }
    },
    "capabilities": [
      "inference"
    ],
    "tags": [
      "pose",
      "utility"
    ],
    "lastUpdated": 1750565092965
  },
  "fal-ai/object-removal": {
    "id": "fal-ai/object-removal",
    "name": "Object Removal",
    "category": "image-to-image",
    "description": "Removes objects and their visual effects using natural language, replacing them with contextually appropriate content",
    "parameters": {
      "image_url": {
        "type": "string",
        "description": "The URL of the image to remove objects from.",
        "required": true
      },
      "prompt": {
        "type": "string",
        "description": "Text description of the object to remove.",
        "required": true
      },
      "model": {
        "type": "string",
        "description": "Model quality setting.",
        "required": false,
        "default": "best_quality",
        "enum": [
          "low_quality",
          "medium_quality",
          "high_quality",
          "best_quality"
        ]
      },
      "mask_expansion": {
        "type": "number",
        "description": "Amount of pixels to expand the mask by. Range: 0-50",
        "required": false,
        "default": 15,
        "min": 0,
        "max": 50
      }
    },
    "capabilities": [
      "object removal",
      "image editing"
    ],
    "tags": [
      "utility",
      "editing"
    ],
    "lastUpdated": 1750565098331
  },
  "fal-ai/hunyuan3d-v21": {
    "id": "fal-ai/hunyuan3d-v21",
    "name": "Hunyuan 3D 2.1",
    "category": "image-to-3d",
    "description": "Hunyuan3D-2.1 is a scalable 3D asset creation system that advances state-of-the-art 3D generation through Physically-Based Rendering (PBR).",
    "parameters": {
      "input_image_url": {
        "type": "string",
        "description": "URL of image to use while generating the 3D model.",
        "required": true,
        "default": null
      },
      "seed": {
        "type": "integer",
        "description": "The same seed and the same prompt given to the same version of the model will output the same image every time.",
        "required": false,
        "default": null
      },
      "num_inference_steps": {
        "type": "integer",
        "description": "Number of inference steps to perform. Default value: `50`",
        "required": false,
        "default": 50,
        "min": 1,
        "max": 50
      },
      "guidance_scale": {
        "type": "float",
        "description": "Guidance scale for the model. Default value: `7.5`",
        "required": false,
        "default": 7.5,
        "min": 0,
        "max": 20
      },
      "octree_resolution": {
        "type": "integer",
        "description": "Octree resolution for the model. Default value: `256`",
        "required": false,
        "default": 256,
        "min": 1,
        "max": 1024
      },
      "textured_mesh": {
        "type": "boolean",
        "description": "If set true, textured mesh will be generated and the price charged would be 3 times that of white mesh.",
        "required": false,
        "default": false
      }
    },
    "capabilities": [
      "3D asset creation",
      "Physically-Based Rendering (PBR)"
    ],
    "tags": [
      "image-to-3d"
    ],
    "lastUpdated": 1750565099060
  },
  "fal-ai/wan-trainer": {
    "id": "fal-ai/wan-trainer",
    "name": "Wan-2.1 LoRA Trainer",
    "category": "training",
    "description": "Train custom LoRAs for Wan-2.1 I2V 480P",
    "parameters": {
      "training_data_url": {
        "type": "string",
        "description": "URL to zip archive with images of a consistent style. Try to use at least 10 images and/or videos, although more is better. In addition to images the archive can contain text files with captions. Each text file should have the same name as the image/video file it corresponds to.",
        "required": true
      },
      "number_of_steps": {
        "type": "integer",
        "description": "The number of steps to train for. Default value: `400`",
        "required": false,
        "default": 400,
        "min": 1,
        "max": 20000
      },
      "learning_rate": {
        "type": "float",
        "description": "The rate at which the model learns. Higher values can lead to faster training, but over-fitting. Default value: `0.0002`",
        "required": false,
        "default": 0.0002,
        "min": 0.000001,
        "max": 1
      },
      "trigger_phrase": {
        "type": "string",
        "description": "The phrase that will trigger the model to generate an image. Default value: `\"\"`",
        "required": false,
        "default": ""
      },
      "auto_scale_input": {
        "type": "boolean",
        "description": "If true, the input will be automatically scale the video to 81 frames at 16fps.",
        "required": false,
        "default": false
      }
    },
    "capabilities": [
      "training",
      "commercial use"
    ],
    "tags": [
      "lora",
      "training"
    ],
    "lastUpdated": 1750565101737
  },
  "fal-ai/pasd": {
    "id": "fal-ai/pasd",
    "name": "PASD",
    "category": "image-to-image",
    "description": "Pixel-Aware Diffusion Model for Realistic Image Super-Resolution and Personalized Stylization",
    "parameters": {
      "image_url": {
        "type": "string",
        "description": "Input image to super-resolve",
        "required": true,
        "default": "https://fal.media/files/rabbit/JlBgYUyQRS3zxiBu_B4fM.png"
      },
      "scale": {
        "type": "integer",
        "description": "Upscaling factor (1-4x)",
        "required": false,
        "default": 2,
        "min": 1,
        "max": 4
      },
      "steps": {
        "type": "integer",
        "description": "Number of inference steps (10-50)",
        "required": false,
        "default": 25,
        "min": 10,
        "max": 50
      },
      "guidance_scale": {
        "type": "number",
        "description": "Guidance scale for diffusion (1.0-20.0)",
        "required": false,
        "default": 7,
        "min": 1,
        "max": 20
      },
      "conditioning_scale": {
        "type": "number",
        "description": "ControlNet conditioning scale (0.1-1.0)",
        "required": false,
        "default": 0.8,
        "min": 0.1,
        "max": 1
      },
      "prompt": {
        "type": "string",
        "description": "Additional prompt to guide super-resolution",
        "required": false,
        "default": ""
      },
      "negative_prompt": {
        "type": "string",
        "description": "Negative prompt to avoid unwanted artifacts",
        "required": false,
        "default": "blurry, dirty, messy, frames, deformed, dotted, noise, raster lines, unclear, lowres, over-smoothed, painting, ai generated"
      }
    },
    "capabilities": [
      "inference",
      "commercial use"
    ],
    "tags": [
      "utility",
      "editing"
    ],
    "lastUpdated": 1750565107284
  },
  "fal-ai/luma-photon": {
    "id": "fal-ai/luma-photon",
    "name": "Luma Photon",
    "category": "text-to-image",
    "description": "Generate images from your prompts using Luma Photon. Photon is the most creative, personalizable, and intelligent visual models for creatives, bringing a step-function change in the cost of high-quality image generation.",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "The prompt for generating the image.",
        "required": true,
        "min": 3,
        "max": 5000
      },
      "aspect_ratio": {
        "type": "string",
        "description": "The aspect ratio of the generated image.",
        "required": false,
        "default": "1:1",
        "enum": [
          "16:9",
          "9:16",
          "1:1",
          "4:3",
          "3:4",
          "21:9",
          "9:21"
        ]
      }
    },
    "capabilities": [
      "text-to-image"
    ],
    "tags": [
      "inference",
      "commercial",
      "partner"
    ],
    "lastUpdated": 1750565122632
  },
  "fal-ai/llm-nsfw-checker-shopify": {
    "id": "fal-ai/llm-nsfw-checker-shopify",
    "name": "Llm Nsfw Checker Shopify",
    "category": "llm",
    "description": "llm nsfw detector",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "The prompt to check and respond to",
        "required": true,
        "default": ""
      },
      "model": {
        "type": "enum",
        "description": "Model to use",
        "required": false,
        "default": "google/gemini-2.0-flash-001",
        "enum": [
          "google/gemini-2.0-flash-001",
          "anthropic/claude-3.5-sonnet",
          "google/gemini-flash-1.5",
          "openai/gpt-4o-mini",
          "openai/gpt-4o",
          "meta-llama/llama-3.1-8b-instruct",
          "meta-llama/llama-3.1-70b-instruct"
        ]
      },
      "system_prompt": {
        "type": "string",
        "description": "System prompt for NSFW checking",
        "required": false,
        "default": "You are a specialized content filter for an AI image generation tool. Your task is to analyze user prompts and identify illegal or harmful content.\nINSTRUCTIONS:\nExamine the provided image prompt carefully for illegal/harmful content:\nILLEGAL/HARMFUL CONTENT CATEGORIES:\n- CSAM or sexualization of minors (any content involving children in sexual contexts)\n- Explicit sexual content (pornography, graphic sexual acts)\n- FTO/terrorist content (extremist flags, terrorist propaganda, extremist symbols)\n- Illegal drugs (drug manufacturing, selling, or glorification)\n- Extreme violence (graphic violence, gore, torture)\n- Suicide (methods, encouragement, glorification)\n- Clearly fraudulent content (counterfeit documents, scams)\nOUTPUT FORMAT:\n- If any illegal/harmful violation detected: YES\n- If no illegal/harmful violations detected: NO\nIMPORTANT CONSIDERATIONS:\n- Be vigilant about misspellings or creative workarounds\n- When in doubt about whether something violates policy, err on the side of caution\nExamples of prompts to block:\n- \"Young girl in suggestive pose\" (potential minor sexualization)\n- \"How to make crystal meth step by step\" (illegal drugs)\n- \"Realistic extremist organization flag waving\" (terrorist content)\n- \"Graphic depiction of suicide method\" (suicide content)"
      }
    },
    "capabilities": [
      "Inference",
      "Commercial use"
    ],
    "tags": [],
    "lastUpdated": 1750565133712
  },
  "fal-ai/luma-dream-machine": {
    "id": "fal-ai/luma-dream-machine",
    "name": "Luma Dream Machine",
    "category": "text-to-video",
    "description": "Generate video clips from your prompts using Luma Dream Machine v1.5",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "The text prompt to generate the video from",
        "required": true,
        "max": 5000
      },
      "aspect_ratio": {
        "type": "string",
        "description": "The aspect ratio of the generated video",
        "required": false,
        "default": "16:9",
        "enum": [
          "16:9",
          "9:16",
          "4:3",
          "3:4",
          "21:9",
          "9:21"
        ]
      },
      "loop": {
        "type": "boolean",
        "description": "Whether the video should loop (end of video is blended with the beginning)",
        "required": false,
        "default": false
      }
    },
    "capabilities": [
      "text-to-video",
      "video generation"
    ],
    "tags": [
      "motion",
      "transformation"
    ],
    "lastUpdated": 1750565178959
  },
  "fal-ai/flux-1": {
    "id": "fal-ai/flux-1",
    "name": "Flux 1",
    "category": "unknown",
    "description": "FLUX.1 [dev], next generation text-to-image model.",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "The prompt to generate an image from.",
        "required": true
      },
      "image_size": {
        "type": "string",
        "description": "The size of the generated image. Default value: `landscape_4_3`",
        "required": false,
        "default": "landscape_4_3",
        "enum": [
          "square_hd",
          "square",
          "portrait_4_3",
          "portrait_16_9",
          "landscape_4_3",
          "landscape_16_9"
        ]
      },
      "num_inference_steps": {
        "type": "number",
        "description": "The number of inference steps to perform. Default value: `28`",
        "required": false,
        "default": 28,
        "min": 1,
        "max": 50
      },
      "seed": {
        "type": "number",
        "description": "The same seed and the same prompt given to the same version of the model will output the same image every time.",
        "required": false
      },
      "guidance_scale": {
        "type": "number",
        "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`",
        "required": false,
        "default": 3.5,
        "min": 1,
        "max": 20
      },
      "sync_mode": {
        "type": "boolean",
        "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.",
        "required": false,
        "default": false
      },
      "num_images": {
        "type": "number",
        "description": "The number of images to generate. Default value: `1`",
        "required": false,
        "default": 1,
        "min": 1,
        "max": 4
      },
      "enable_safety_checker": {
        "type": "boolean",
        "description": "If set to true, the safety checker will be enabled. Default value: `true`",
        "required": false,
        "default": true
      }
    },
    "capabilities": [
      "text-to-image",
      "image-to-image",
      "streaming"
    ],
    "tags": [
      "inference",
      "private"
    ],
    "lastUpdated": 1750565187509
  },
  "fal-ai/hunyuan-portrait": {
    "id": "fal-ai/hunyuan-portrait",
    "name": "Hunyuan Portrait",
    "category": "image-to-video",
    "description": "HunyuanPortrait is a diffusion-based framework for generating lifelike, temporally consistent portrait animations.",
    "parameters": {
      "video_url": {
        "type": "string",
        "description": "The URL of the driving video.",
        "required": true
      },
      "image_url": {
        "type": "string",
        "description": "The URL of the source image.",
        "required": true
      },
      "seed": {
        "type": "integer",
        "description": "Random seed for generation. If None, a random seed will be used.",
        "required": false
      },
      "use_arcface": {
        "type": "boolean",
        "description": "Whether to use ArcFace for face recognition.",
        "required": false,
        "default": true
      }
    },
    "capabilities": [
      "animation",
      "lip sync"
    ],
    "tags": [
      "animation",
      "lip sync"
    ],
    "lastUpdated": 1750565205117
  },
  "fal-ai/hunyuan-avatar": {
    "id": "fal-ai/hunyuan-avatar",
    "name": "Hunyuan Avatar",
    "category": "image-to-video",
    "description": "HunyuanAvatar is a High-Fidelity Audio-Driven Human Animation model for Multiple Characters.",
    "parameters": {
      "audio_url": {
        "type": "string",
        "description": "The URL of the audio file.",
        "required": true
      },
      "image_url": {
        "type": "string",
        "description": "The URL of the reference image.",
        "required": true
      },
      "text": {
        "type": "string",
        "description": "Text prompt describing the scene.",
        "required": false,
        "default": "A girl is singing."
      },
      "num_inference_steps": {
        "type": "number",
        "description": "Number of inference steps for sampling. Higher values give better quality but take longer.",
        "required": false,
        "default": 30,
        "min": 30,
        "max": 50
      },
      "turbo_mode": {
        "type": "boolean",
        "description": "If true, the video will be generated faster with no noticeable degradation in the visual quality.",
        "required": false,
        "default": true
      },
      "seed": {
        "type": "number",
        "description": "Random seed for generation.",
        "required": false
      }
    },
    "capabilities": [
      "inference",
      "commercial use"
    ],
    "tags": [
      "stylized",
      "transform"
    ],
    "lastUpdated": 1750565205861
  },
  "fal-ai/flux-kontext-dev": {
    "id": "fal-ai/flux-kontext-dev",
    "name": "Flux Kontext Dev",
    "category": "image-to-image",
    "description": "Frontier image editing model.",
    "parameters": {
      "image_url": {
        "type": "string",
        "description": "The URL of the image to edit.",
        "required": true
      },
      "prompt": {
        "type": "string",
        "description": "The prompt to edit the image.",
        "required": true
      },
      "num_inference_steps": {
        "type": "number",
        "description": "The number of inference steps to perform.",
        "required": false,
        "default": 30,
        "min": 10,
        "max": 50
      },
      "seed": {
        "type": "number",
        "description": "The same seed and the same prompt given to the same version of the model will output the same image every time.",
        "required": false
      },
      "guidance_scale": {
        "type": "number",
        "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.",
        "required": false,
        "default": 2.5,
        "min": 1,
        "max": 20
      },
      "sync_mode": {
        "type": "boolean",
        "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.",
        "required": false,
        "default": false
      },
      "num_images": {
        "type": "number",
        "description": "The number of images to generate.",
        "required": false,
        "default": 1,
        "min": 1,
        "max": 4
      },
      "enable_safety_checker": {
        "type": "boolean",
        "description": "If set to true, the safety checker will be enabled.",
        "required": false,
        "default": true
      }
    },
    "capabilities": [
      "image editing",
      "image generation"
    ],
    "tags": [
      "inference",
      "private"
    ],
    "lastUpdated": 1750565215041
  },
  "fal-ai/lyria2": {
    "id": "fal-ai/lyria2",
    "name": "Lyria2",
    "category": "text-to-audio",
    "description": "Lyria 2 is Google's latest music generation model, you can generate any type of music with this model.",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "The text prompt describing the music you want to generate",
        "required": true
      },
      "seed": {
        "type": "integer",
        "description": "A seed for deterministic generation. If provided, the model will attempt to produce the same audio given the same prompt and other parameters.",
        "required": false
      },
      "negative_prompt": {
        "type": "string",
        "description": "A description of what to exclude from the generated audio",
        "required": false
      }
    },
    "capabilities": [
      "Generate music using Google's Lyria 2 text-to-music model",
      "48kHz WAV audio output",
      "30 second duration",
      "Negative prompting to exclude unwanted elements",
      "Seed-based reproducibility"
    ],
    "tags": [
      "music",
      "stylized"
    ],
    "lastUpdated": 1750565238990
  },
  "fal-ai/bagel": {
    "id": "fal-ai/bagel",
    "name": "Bagel",
    "category": "text-to-image",
    "description": "Bagel is a 7B parameter from Bytedance-Seed multimodal model that can generate both text and images.",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "The prompt to generate an image from.",
        "required": true
      },
      "seed": {
        "type": "integer",
        "description": "The seed to use for the generation.",
        "required": false
      },
      "use_thought": {
        "type": "boolean",
        "description": "Whether to use thought tokens for generation. If set to true, the model will \"think\" to potentially improve generation quality. Increases generation time and increases the cost by 20%.",
        "required": false,
        "default": false
      },
      "enable_safety_checker": {
        "type": "boolean",
        "description": "If set to true, the safety checker will be enabled.",
        "required": false,
        "default": true
      }
    },
    "capabilities": [
      "text-to-image",
      "multimodal"
    ],
    "tags": [
      "text-to-image",
      "multimodal"
    ],
    "lastUpdated": 1750565277251
  },
  "fal-ai/wan-t2i": {
    "id": "fal-ai/wan-t2i",
    "name": "Wan-2.1 Text-to-Image",
    "category": "text-to-image",
    "description": "Generate images using Wan-2.1 14B",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "Prompt to generate the image from",
        "required": true
      },
      "negative_prompt": {
        "type": "string",
        "description": "Negative prompt to avoid in the generated image",
        "required": false,
        "default": "low quality, bad anatomy, worst quality, lowres, jpeg artifacts, signature, watermark, blurry"
      },
      "width": {
        "type": "integer",
        "description": "Width of the generated image",
        "required": false,
        "default": 1024,
        "min": 32,
        "max": 2048
      },
      "height": {
        "type": "integer",
        "description": "Height of the generated image",
        "required": false,
        "default": 1024,
        "min": 32,
        "max": 2048
      },
      "num_inference_steps": {
        "type": "integer",
        "description": "Number of steps to generate the image",
        "required": false,
        "default": 28,
        "min": 1,
        "max": 50
      },
      "num_images": {
        "type": "integer",
        "description": "Number of images to generate",
        "required": false,
        "default": 1,
        "min": 1,
        "max": 4
      },
      "guidance_scale": {
        "type": "float",
        "description": "Guidance scale for the generation",
        "required": false,
        "default": 5,
        "min": 1,
        "max": 12
      },
      "seed": {
        "type": "integer",
        "description": "Seed for random number generation. If not provided, a random seed will be used.",
        "required": false
      },
      "loras": {
        "type": "array",
        "description": "List of LoRA weights to use for generation. If not provided, no LoRA weights will be used.",
        "required": false,
        "default": []
      },
      "output_format": {
        "type": "string",
        "description": "Output format of the image. Default is jpeg.",
        "required": false,
        "default": "jpeg",
        "enum": [
          "jpeg",
          "png"
        ]
      },
      "sync_mode": {
        "type": "boolean",
        "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.",
        "required": false,
        "default": false
      },
      "use_finetune": {
        "type": "boolean",
        "description": "If set to true, the function will use the finetuned model, otherwise it will use the base model.",
        "required": false,
        "default": false
      }
    },
    "capabilities": [
      "inference"
    ],
    "tags": [
      "wan",
      "text-to-image"
    ],
    "lastUpdated": 1750565302475
  },
  "fal-ai/video-depth-anything": {
    "id": "fal-ai/video-depth-anything",
    "name": "Video Depth Anything",
    "category": "video-to-video",
    "description": "Video Depth Anything can be applied to arbitrarily long videos without compromising quality, consistency, or generalization ability. Compared with other diffusion-based models, it enjoys faster inference speed, fewer parameters, and higher consistent depth accuracy.",
    "parameters": {
      "video_url": {
        "type": "string",
        "description": "URL of video to be used for depth map creation.",
        "required": true,
        "default": ""
      },
      "seed": {
        "type": "integer",
        "description": "Random seed for reproducible generation.",
        "required": false,
        "default": null
      }
    },
    "capabilities": [
      "inference",
      "commercial use"
    ],
    "tags": [
      "video-to-video"
    ],
    "lastUpdated": 1750565313479
  },
  "fal-ai/uni-animate": {
    "id": "fal-ai/uni-animate",
    "name": "Uni Animate",
    "category": "image-to-video",
    "description": "It takes human image animation to a whole new level! It's basically built on this cracked DiT model called Wan2.1-14B-I2V that makes your still pics move in ways that actually look legit.",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "The text prompt to guide video generation.",
        "required": true
      },
      "negative_prompt": {
        "type": "string",
        "description": "Negative prompt for video generation. Default value: 'bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards'",
        "required": false,
        "default": "bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards"
      },
      "image_url": {
        "type": "string",
        "description": "URL to the source video file. If provided, the model will use this video as a reference.",
        "required": true
      },
      "video_url": {
        "type": "string",
        "description": "URL to the source video file. If provided, the model will use this video as a reference.",
        "required": true
      },
      "resolution": {
        "type": "string",
        "description": "Resolution of the output video.",
        "required": false,
        "default": "480p",
        "enum": [
          "480p",
          "720p"
        ]
      },
      "seed": {
        "type": "integer",
        "description": "Random seed for reproducibility. If None, a random seed is chosen.",
        "required": false
      },
      "num_inference_steps": {
        "type": "integer",
        "description": "Number of inference steps for sampling. Higher values give better quality but take longer.",
        "required": false,
        "default": 30,
        "min": 2,
        "max": 50
      },
      "cfg_scale": {
        "type": "number",
        "description": "CFG scale for the model.",
        "required": false,
        "default": 1.5,
        "min": 1,
        "max": 5
      }
    },
    "capabilities": [
      "inference"
    ],
    "tags": [
      "image-to-video"
    ],
    "lastUpdated": 1750565318765
  },
  "fal-ai/ltx-video-13b-distilled": {
    "id": "fal-ai/ltx-video-13b-distilled",
    "name": "LTX Video-0.9.7 13B Distilled",
    "category": "text-to-video",
    "description": "Generate videos from prompts using LTX Video-0.9.7 13B Distilled and custom LoRA",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "Text prompt to guide generation",
        "required": true
      },
      "negative_prompt": {
        "type": "string",
        "description": "Negative prompt for generation",
        "required": false,
        "default": "worst quality, inconsistent motion, blurry, jittery, distorted"
      },
      "loras": {
        "type": "array",
        "description": "LoRA weights to use for generation",
        "required": false,
        "default": []
      },
      "resolution": {
        "type": "string",
        "description": "Resolution of the generated video (480p or 720p)",
        "required": false,
        "default": "720p",
        "enum": [
          "480p",
          "720p"
        ]
      },
      "aspect_ratio": {
        "type": "string",
        "description": "Aspect ratio of the generated video (16:9, 1:1 or 9:16)",
        "required": false,
        "default": "16:9",
        "enum": [
          "9:16",
          "1:1",
          "16:9"
        ]
      },
      "seed": {
        "type": "integer",
        "description": "Random seed for generation",
        "required": false
      },
      "number_of_frames": {
        "type": "integer",
        "description": "The number of frames in the video",
        "required": false,
        "default": 121,
        "min": 9,
        "max": 161
      },
      "first_pass_number_of_steps": {
        "type": "integer",
        "description": "Number of inference steps during the first pass",
        "required": false,
        "default": 8,
        "min": 2,
        "max": 20
      },
      "first_pass_skip_final_steps": {
        "type": "integer",
        "description": "Number of inference steps to skip in the final steps of the first pass",
        "required": false,
        "default": 1,
        "min": 0,
        "max": 20
      },
      "second_pass_number_of_steps": {
        "type": "integer",
        "description": "Number of inference steps during the second pass",
        "required": false,
        "default": 8,
        "min": 2,
        "max": 20
      },
      "second_pass_skip_initial_st steps": {
        "type": "integer",
        "description": "The number of inference steps to skip in the initial steps of the second pass",
        "required": false,
        "default": 5,
        "min": 1,
        "max": 20
      },
      "frame_rate": {
        "type": "integer",
        "description": "The frame rate of the video",
        "required": false,
        "default": 30,
        "min": 1,
        "max": 60
      },
      "expand_prompt": {
        "type": "boolean",
        "description": "Whether to expand the prompt using a language model",
        "required": false,
        "default": false
      },
      "reverse_video": {
        "type": "boolean",
        "description": "Whether to reverse the video",
        "required": false,
        "default": false
      },
      "enable_safety_checker": {
        "type": "boolean",
        "description": "Whether to enable the safety checker",
        "required": false,
        "default": true
      }
    },
    "capabilities": [
      "text-to-video"
    ],
    "tags": [
      "video",
      "ltx-video",
      "text-to-video"
    ],
    "lastUpdated": 1750565342983
  },
  "fal-ai/ltx-video-13b-dev": {
    "id": "fal-ai/ltx-video-13b-dev",
    "name": "LTX Video-0.9.7 13B",
    "category": "text-to-video",
    "description": "Generate videos from prompts using LTX Video-0.9.7 13B and custom LoRA",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "Text prompt to guide generation",
        "required": true
      },
      "negative_prompt": {
        "type": "string",
        "description": "Negative prompt for generation",
        "required": false,
        "default": "worst quality, inconsistent motion, blurry, jittery, distorted"
      },
      "loras": {
        "type": "array",
        "description": "LoRA weights to use for generation",
        "required": false,
        "default": []
      },
      "resolution": {
        "type": "string",
        "description": "Resolution of the generated video (480p or 720p)",
        "required": false,
        "default": "720p",
        "enum": [
          "480p",
          "720p"
        ]
      },
      "aspect_ratio": {
        "type": "string",
        "description": "Aspect ratio of the generated video (16:9, 1:1 or 9:16)",
        "required": false,
        "default": "16:9",
        "enum": [
          "9:16",
          "1:1",
          "16:9"
        ]
      },
      "seed": {
        "type": "integer",
        "description": "Random seed for generation",
        "required": false
      },
      "number_of_frames": {
        "type": "integer",
        "description": "The number of frames in the video",
        "required": false,
        "default": 121,
        "min": 9,
        "max": 161
      },
      "first_pass_number_of_steps": {
        "type": "integer",
        "description": "Number of inference steps during the first pass",
        "required": false,
        "default": 30,
        "min": 2,
        "max": 50
      },
      "first_pass_skip_final_steps": {
        "type": "integer",
        "description": "Number of inference steps to skip in the final steps of the first pass",
        "required": false,
        "default": 3,
        "min": 0,
        "max": 50
      },
      "second_pass_number_of_steps": {
        "type": "integer",
        "description": "Number of inference steps during the second pass",
        "required": false,
        "default": 30,
        "min": 2,
        "max": 50
      },
      "second_pass_skip_initial_steps": {
        "type": "integer",
        "description": "The number of inference steps to skip in the initial steps of the second pass",
        "required": false,
        "default": 17,
        "min": 1,
        "max": 50
      },
      "frame_rate": {
        "type": "integer",
        "description": "The frame rate of the video",
        "required": false,
        "default": 30,
        "min": 1,
        "max": 60
      },
      "expand_prompt": {
        "type": "boolean",
        "description": "Whether to expand the prompt using a language model",
        "required": false,
        "default": false
      },
      "reverse_video": {
        "type": "boolean",
        "description": "Whether to reverse the video",
        "required": false,
        "default": false
      },
      "enable_safety_checker": {
        "type": "boolean",
        "description": "Whether to enable the safety checker",
        "required": false,
        "default": true
      }
    },
    "capabilities": [
      "text-to-video",
      "video generation"
    ],
    "tags": [
      "video",
      "ltx-video",
      "text-to-video"
    ],
    "lastUpdated": 1750565345875
  },
  "fal-ai/dreamo": {
    "id": "fal-ai/dreamo",
    "name": "DreamO",
    "category": "text-to-image",
    "description": "DreamO is an image customization framework designed to support a wide range of tasks while facilitating seamless integration of multiple conditions.",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "The prompt to generate an image from.",
        "required": true
      },
      "first_image_url": {
        "type": "string",
        "description": "URL of first reference image to use for generation.",
        "required": false
      },
      "second_image_url": {
        "type": "string",
        "description": "URL of second reference image to use for generation.",
        "required": false
      },
      "first_reference_task": {
        "type": "enum",
        "description": "Task for first reference image (ip/id/style).",
        "required": false,
        "default": "ip",
        "enum": [
          "ip",
          "id",
          "style"
        ]
      },
      "second_reference_task": {
        "type": "enum",
        "description": "Task for second reference image (ip/id/style).",
        "required": false,
        "default": "ip",
        "enum": [
          "ip",
          "id",
          "style"
        ]
      },
      "image_size": {
        "type": "enum",
        "description": "The size of the generated image.",
        "required": false,
        "default": "square_hd",
        "enum": [
          "square_hd",
          "square",
          "portrait_4_3",
          "portrait_16_9",
          "landscape_4_3",
          "landscape_16_9"
        ]
      },
      "num_inference_steps": {
        "type": "number",
        "description": "The number of inference steps to perform.",
        "required": false,
        "default": 12,
        "min": 1,
        "max": 50
      },
      "guidance_scale": {
        "type": "number",
        "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.",
        "required": false,
        "default": 3.5,
        "min": 0,
        "max": 20
      },
      "negative_prompt": {
        "type": "string",
        "description": "The prompt to generate an image from.",
        "required": false,
        "default": ""
      },
      "sync_mode": {
        "type": "boolean",
        "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response.",
        "required": false,
        "default": false
      },
      "ref_resolution": {
        "type": "number",
        "description": "Resolution for reference images.",
        "required": false,
        "default": 512,
        "min": 512,
        "max": 1024
      },
      "true_cfg": {
        "type": "number",
        "description": "The weight of the CFG loss.",
        "required": false,
        "default": 1,
        "min": 1,
        "max": 5
      },
      "enable_safety_checker": {
        "type": "boolean",
        "description": "If set to true, the safety checker will be enabled.",
        "required": false,
        "default": true
      }
    },
    "capabilities": [
      "image generation",
      "image customization"
    ],
    "tags": [
      "stylized",
      "realism"
    ],
    "lastUpdated": 1750565375002
  },
  "fal-ai/ace-step": {
    "id": "fal-ai/ace-step",
    "name": "ACE-Step",
    "category": "text-to-audio",
    "description": "Generate music with lyrics from text using ACE-Step",
    "parameters": {
      "tags": {
        "type": "string",
        "description": "Comma-separated list of genre tags to control the style of the generated audio.",
        "required": true,
        "default": "lofi, hiphop, drum and bass, trap, chill"
      },
      "lyrics": {
        "type": "string",
        "description": "Lyrics to be sung in the audio. If not provided or if [inst] or [instrumental] is the content of this field, no lyrics will be sung. Use control structures like [verse], [chorus] and [bridge] to control the structure of the song.",
        "required": false,
        "default": ""
      },
      "duration": {
        "type": "number",
        "description": "The duration of the generated audio in seconds.",
        "required": false,
        "default": 60,
        "min": 5,
        "max": 240
      },
      "number_of_steps": {
        "type": "integer",
        "description": "Number of steps to generate the audio.",
        "required": false,
        "default": 27,
        "min": 3,
        "max": 60
      },
      "seed": {
        "type": "integer",
        "description": "Random seed for reproducibility. If not provided, a random seed will be used.",
        "required": false
      },
      "scheduler": {
        "type": "string",
        "description": "Scheduler to use for the generation process.",
        "required": false,
        "default": "euler",
        "enum": [
          "euler",
          "heun"
        ]
      },
      "guidance_type": {
        "type": "string",
        "description": "Type of CFG to use for the generation process.",
        "required": false,
        "default": "apg",
        "enum": [
          "cfg",
          "apg",
          "cfg_star"
        ]
      },
      "granularity_scale": {
        "type": "integer",
        "description": "Granularity scale for the generation process. Higher values can reduce artifacts.",
        "required": false,
        "default": 10,
        "min": -100,
        "max": 100
      },
      "guidance_interval": {
        "type": "number",
        "description": "Guidance interval for the generation. 0.5 means only apply guidance in the middle steps (0.25 * infer_steps to 0.75 * infer_steps).",
        "required": false,
        "default": 0.5,
        "min": 0,
        "max": 1
      },
      "guidance_interval_decay": {
        "type": "number",
        "description": "Guidance interval decay for the generation. Guidance scale will decay from guidance_scale to min_guidance_scale in the interval. 0.0 means no decay.",
        "required": false,
        "default": 0,
        "min": 0,
        "max": 1
      },
      "guidance_scale": {
        "type": "number",
        "description": "Guidance scale for the generation.",
        "required": false,
        "default": 15,
        "min": 0,
        "max": 200
      },
      "minimum_guidance_scale": {
        "type": "number",
        "description": "Minimum guidance scale for the generation after the decay.",
        "required": false,
        "default": 3,
        "min": 0,
        "max": 200
      },
      "tag_guidance_scale": {
        "type": "number",
        "description": "Tag guidance scale for the generation.",
        "required": false,
        "default": 5,
        "min": 0,
        "max": 10
      },
      "lyric_guidance_scale": {
        "type": "number",
        "description": "Lyric guidance scale for the generation.",
        "required": false,
        "default": 1.5,
        "min": 0,
        "max": 10
      }
    },
    "capabilities": [
      "text-to-audio",
      "text-to-music"
    ],
    "tags": [
      "text-to-audio",
      "text-to-music"
    ],
    "lastUpdated": 1750565410416
  },
  "fal-ai/ltx-video-lora": {
    "id": "fal-ai/ltx-video-lora",
    "name": "LTX Video-0.9.7 LoRA",
    "category": "text-to-video",
    "description": "Deprecated. Use fal-ai/ltx-video-13b-dev or fal-ai/ltx-video-13b-distilled instead.",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "The prompt to generate the video from.",
        "required": true
      },
      "negative_prompt": {
        "type": "string",
        "description": "The negative prompt to use. Default value: 'blurry, low quality, low resolution, inconsistent motion, jittery, distorted'",
        "required": false,
        "default": "blurry, low quality, low resolution, inconsistent motion, jittery, distorted"
      },
      "loras": {
        "type": "array",
        "description": "The LoRA weights to use for generation.",
        "required": false,
        "default": []
      },
      "resolution": {
        "type": "string",
        "description": "The resolution of the video. Default value: '720p'",
        "required": false,
        "default": "720p",
        "enum": [
          "480p",
          "720p"
        ]
      },
      "aspect_ratio": {
        "type": "string",
        "description": "The aspect ratio of the video. Default value: '16:9'",
        "required": false,
        "default": "16:9",
        "enum": [
          "16:9",
          "1:1",
          "9:16"
        ]
      },
      "number_of_frames": {
        "type": "number",
        "description": "The number of frames in the video. Default value: 89",
        "required": false,
        "default": 89,
        "min": 9,
        "max": 161
      },
      "number_of_steps": {
        "type": "number",
        "description": "The number of inference steps to use. Default value: 30",
        "required": false,
        "default": 30,
        "min": 1,
        "max": 50
      },
      "frame_rate": {
        "type": "number",
        "description": "The frame rate of the video. Default value: 25",
        "required": false,
        "default": 25,
        "min": 1,
        "max": 60
      },
      "expand_prompt": {
        "type": "boolean",
        "description": "Whether to expand the prompt using the LLM.",
        "required": false,
        "default": false
      },
      "reverse_video": {
        "type": "boolean",
        "description": "Whether to reverse the video.",
        "required": false,
        "default": false
      },
      "enable_safety_checker": {
        "type": "boolean",
        "description": "Whether to enable the safety checker. Default value: true",
        "required": false,
        "default": true
      }
    },
    "capabilities": [
      "text-to-video"
    ],
    "tags": [
      "video",
      "ltx-video",
      "text-to-video"
    ],
    "lastUpdated": 1750565423751
  },
  "fal-ai/hunyuan-custom": {
    "id": "fal-ai/hunyuan-custom",
    "name": "Hunyuan Custom",
    "category": "image-to-video",
    "description": "HunyuanCustom revolutionizes video generation with unmatched identity consistency across multiple input types. Its innovative fusion modules and alignment networks outperform competitors, maintaining subject integrity while responding flexibly to text, image, audio, and video conditions.",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "Text prompt for video generation (max 500 characters).",
        "required": true
      },
      "negative_prompt": {
        "type": "string",
        "description": "Negative prompt for video generation.",
        "required": false,
        "default": "Aerial view, aerial view, overexposed, low quality, deformation, a poor composition, bad hands, bad teeth, bad eyes, bad limbs, distortion, blurring, text, subtitles, static, picture, black border."
      },
      "image_url": {
        "type": "string",
        "description": "URL of the image input.",
        "required": true
      },
      "num_inference_steps": {
        "type": "number",
        "description": "The number of inference steps to run. Lower gets faster results, higher gets better results.",
        "required": false,
        "default": 30,
        "min": 10,
        "max": 30
      },
      "seed": {
        "type": "number",
        "description": "The seed to use for generating the video.",
        "required": false
      },
      "aspect_ratio": {
        "type": "string",
        "description": "The aspect ratio of the video to generate.",
        "required": false,
        "default": "16:9",
        "enum": [
          "16:9",
          "9:16"
        ]
      },
      "resolution": {
        "type": "string",
        "description": "The resolution of the video to generate. 720p generations cost 1.5x more than 480p generations.",
        "required": false,
        "default": "512p",
        "enum": [
          "512p",
          "720p"
        ]
      },
      "fps": {
        "type": "number",
        "description": "The frames per second of the generated video.",
        "required": false,
        "default": 25,
        "min": 16,
        "max": 30
      },
      "cfg_scale": {
        "type": "number",
        "description": "Classifier-Free Guidance scale for the generation.",
        "required": false,
        "default": 7.5,
        "min": 1.5,
        "max": 13
      },
      "num_frames": {
        "type": "number",
        "description": "The number of frames to generate.",
        "required": false,
        "default": 129,
        "min": 81,
        "max": 129
      },
      "enable_prompt_expansion": {
        "type": "boolean",
        "description": "Whether to enable prompt expansion.",
        "required": false,
        "default": true
      },
      "enable_safety_checker": {
        "type": "boolean",
        "description": "If set to true, the safety checker will be enabled.",
        "required": false,
        "default": true
      }
    },
    "capabilities": [
      "video generation",
      "identity consistency",
      "text-to-video",
      "image-to-video",
      "audio-to-video"
    ],
    "tags": [
      "image-to-video"
    ],
    "lastUpdated": 1750565423822
  },
  "fal-ai/ltx-video-v097": {
    "id": "fal-ai/ltx-video-v097",
    "name": "LTX Video-0.9.7",
    "category": "text-to-video",
    "description": "Deprecated. Use fal-ai/ltx-video-13b-dev or fal-ai/ltx-video-13b-distilled instead.",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "Text prompt to guide generation",
        "required": true
      },
      "negative_prompt": {
        "type": "string",
        "description": "Negative prompt for generation",
        "required": false,
        "default": "worst quality, inconsistent motion, blurry, jittery, distorted"
      },
      "resolution": {
        "type": "string",
        "description": "Resolution of the generated video (480p or 720p).",
        "required": false,
        "default": "720p",
        "enum": [
          "480p",
          "720p"
        ]
      },
      "aspect_ratio": {
        "type": "string",
        "description": "Aspect ratio of the generated video (16:9 or 9:16).",
        "required": false,
        "default": "16:9",
        "enum": [
          "9:16",
          "16:9"
        ]
      },
      "num_inference_steps": {
        "type": "number",
        "description": "Number of inference steps",
        "required": false,
        "default": 40,
        "min": 2,
        "max": 50
      },
      "expand_prompt": {
        "type": "boolean",
        "description": "Whether to expand the prompt using the model's own capabilities.",
        "required": false,
        "default": true
      }
    },
    "capabilities": [
      "text-to-video"
    ],
    "tags": [
      "video",
      "text-video"
    ],
    "lastUpdated": 1750565441424
  },
  "fal-ai/ltx-video-v096": {
    "id": "fal-ai/ltx-video-v096",
    "name": "LTX Video-0.9.6",
    "category": "text-to-video",
    "description": "Generate videos from prompts using LTX Video-0.9.6",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "Text prompt to guide generation",
        "required": true
      },
      "negative_prompt": {
        "type": "string",
        "description": "Negative prompt for generation",
        "required": false,
        "default": "worst quality, inconsistent motion, blurry, jittery, distorted"
      },
      "resolution": {
        "type": "string",
        "description": "Resolution of the generated video (480p or 720p).",
        "required": false,
        "default": "720p",
        "enum": [
          "480p",
          "720p"
        ]
      },
      "aspect_ratio": {
        "type": "string",
        "description": "Aspect ratio of the generated video (16:9 or 9:16).",
        "required": false,
        "default": "16:9",
        "enum": [
          "9:16",
          "16:9"
        ]
      },
      "seed": {
        "type": "integer",
        "description": "Random seed for generation",
        "required": false
      },
      "num_inference_steps": {
        "type": "integer",
        "description": "Number of inference steps",
        "required": false,
        "default": 40,
        "min": 2,
        "max": 50
      },
      "expand_prompt": {
        "type": "boolean",
        "description": "Whether to expand the prompt using the model's own capabilities.",
        "required": false,
        "default": true
      }
    },
    "capabilities": [
      "inference",
      "commercial use"
    ],
    "tags": [
      "video",
      "text-video"
    ],
    "lastUpdated": 1750565441719
  },
  "fal-ai/trellis": {
    "id": "fal-ai/trellis",
    "name": "Trellis",
    "category": "image-to-3d",
    "description": "Generate 3D models from your images using Trellis. A native 3D generative model enabling versatile and high-quality 3D asset creation.",
    "parameters": {
      "image_url": {
        "type": "string",
        "description": "URL of the input image to convert to 3D",
        "required": true
      },
      "seed": {
        "type": "integer",
        "description": "Random seed for reproducibility",
        "required": false
      },
      "ss_guidance_strength": {
        "type": "number",
        "description": "Guidance strength for sparse structure generation",
        "required": false,
        "default": 7.5,
        "min": 0,
        "max": 10
      },
      "ss_sampling_steps": {
        "type": "integer",
        "description": "Sampling steps for sparse structure generation",
        "required": false,
        "default": 12,
        "min": 1,
        "max": 50
      },
      "slat_guidance_strength": {
        "type": "number",
        "description": "Guidance strength for structured latent generation",
        "required": false,
        "default": 3,
        "min": 0,
        "max": 10
      },
      "slat_sampling_steps": {
        "type": "integer",
        "description": "Sampling steps for structured latent generation",
        "required": false,
        "default": 12,
        "min": 1,
        "max": 50
      },
      "mesh_simplify": {
        "type": "number",
        "description": "Mesh simplification factor",
        "required": false,
        "default": 0.95,
        "min": 0.9,
        "max": 0.98
      },
      "texture_size": {
        "type": "integer",
        "description": "Texture resolution",
        "required": false,
        "default": 1024,
        "enum": [
          512,
          1024,
          2048
        ]
      }
    },
    "capabilities": [
      "3D model generation",
      "image-to-3D conversion"
    ],
    "tags": [
      "stylized"
    ],
    "lastUpdated": 1750565443189
  },
  "fal-ai/pony-v7": {
    "id": "fal-ai/pony-v7",
    "name": "Pony V7",
    "category": "text-to-image",
    "description": "Pony V7 is a finetuned text to image for superior aesthetics and prompt following.",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "The prompt to generate images from",
        "required": true
      },
      "image_size": {
        "type": "string",
        "description": "The size of the generated image. Default value: `square_hd`",
        "required": false,
        "default": "square_hd",
        "enum": [
          "square_hd",
          "square",
          "portrait_4_3",
          "portrait_16_9",
          "landscape_4_3",
          "landscape_16_9"
        ]
      },
      "num_images": {
        "type": "number",
        "description": "The number of images to generate. Default value: `1`",
        "required": false,
        "default": 1,
        "min": 1,
        "max": 2
      },
      "guidance_scale": {
        "type": "number",
        "description": "Classifier free guidance scale. Default value: `3.5`",
        "required": false,
        "default": 3.5,
        "min": 0,
        "max": 20
      },
      "num_inference_steps": {
        "type": "number",
        "description": "The number of inference steps to take. Default value: `40`",
        "required": false,
        "default": 40,
        "min": 20,
        "max": 50
      },
      "enable_safety_checker": {
        "type": "boolean",
        "description": "If set to true, the safety checker will be enabled.",
        "required": false,
        "default": false
      },
      "output_format": {
        "type": "string",
        "description": "The format of the generated image. Default value: `jpeg`",
        "required": false,
        "default": "jpeg",
        "enum": [
          "jpeg",
          "png"
        ]
      }
    },
    "capabilities": [
      "text-to-image",
      "streaming"
    ],
    "tags": [
      "diffusion",
      "style"
    ],
    "lastUpdated": 1750565450832
  },
  "fal-ai/moondream2": {
    "id": "fal-ai/moondream2",
    "name": "Moondream2",
    "category": "vision",
    "description": "Moondream2 is a highly efficient open-source vision language model that combines powerful image understanding capabilities with a remarkably small footprint.",
    "parameters": {
      "image_url": {
        "type": "string",
        "description": "URL of the image to be processed",
        "required": true,
        "default": "https://llava-vl.github.io/static/images/monalisa.jpg"
      }
    },
    "capabilities": [
      "image understanding",
      "captioning"
    ],
    "tags": [
      "Vision"
    ],
    "lastUpdated": 1750565470193
  },
  "fal-ai/hidream-e1-full": {
    "id": "fal-ai/hidream-e1-full",
    "name": "Hidream E1 Full",
    "category": "image-to-image",
    "description": "Edit images with natural language",
    "parameters": {
      "edit_instruction": {
        "type": "string",
        "description": "The instruction to edit the image.",
        "required": false,
        "default": ""
      },
      "target_image_description": {
        "type": "string",
        "description": "The description of the target image after your edits have been made. Leave this blank to allow the model to use its own imagination.",
        "required": false,
        "default": ""
      },
      "negative_prompt": {
        "type": "string",
        "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution).",
        "required": false,
        "default": "low resolution, blur"
      },
      "image_url": {
        "type": "string",
        "description": "URL of an input image to edit.",
        "required": true,
        "default": ""
      },
      "num_inference_steps": {
        "type": "number",
        "description": "The number of inference steps to perform.",
        "required": false,
        "default": 50,
        "min": 1,
        "max": 50
      },
      "seed": {
        "type": "number",
        "description": "The same seed and the same prompt given to the same version of the model will output the same image every time.",
        "required": false,
        "default": ""
      },
      "guidance_scale": {
        "type": "number",
        "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.",
        "required": false,
        "default": 3.5,
        "min": 0,
        "max": 20
      },
      "image_guidance_scale": {
        "type": "number",
        "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your initial image when looking for a related image to show you.",
        "required": false,
        "default": 2,
        "min": 0,
        "max": 20
      },
      "sync_mode": {
        "type": "boolean",
        "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.",
        "required": false,
        "default": false
      },
      "num_images": {
        "type": "number",
        "description": "The number of images to generate.",
        "required": false,
        "default": 1,
        "min": 1,
        "max": 4
      },
      "enable_safety_checker": {
        "type": "boolean",
        "description": "If set to true, the safety checker will be enabled.",
        "required": false,
        "default": true
      },
      "output_format": {
        "type": "string",
        "description": "The format of the generated image.",
        "required": false,
        "default": "jpeg",
        "enum": [
          "jpeg",
          "png"
        ]
      }
    },
    "capabilities": [
      "inference",
      "streaming"
    ],
    "tags": [
      "image-editing",
      "natural-language"
    ],
    "lastUpdated": 1750565484384
  },
  "fal-ai/step1x-edit": {
    "id": "fal-ai/step1x-edit",
    "name": "Step1X Edit",
    "category": "image-to-image",
    "description": "Step1X-Edit transforms your photos with simple instructions into stunning, professional-quality edits—rivaling top proprietary tools.",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "The prompt to generate an image from.",
        "required": true,
        "default": ""
      },
      "image_url": {
        "type": "string",
        "description": "The image URL to generate an image from. Needs to match the dimensions of the mask.",
        "required": true,
        "default": ""
      },
      "negative_prompt": {
        "type": "string",
        "description": "The negative prompt to use. Use it to address details that you don't want in the image.",
        "required": false,
        "default": ""
      },
      "seed": {
        "type": "integer",
        "description": "The same seed and the same prompt given to the same version of the model will output the same image every time.",
        "required": false
      },
      "guidance_scale": {
        "type": "float",
        "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt.",
        "required": false,
        "default": 4,
        "min": 0,
        "max": 20
      },
      "num_inference_steps": {
        "type": "integer",
        "description": "The number of inference steps to perform.",
        "required": false,
        "default": 30,
        "min": 1,
        "max": 50
      },
      "sync_mode": {
        "type": "boolean",
        "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response.",
        "required": false,
        "default": false
      },
      "enable_safety_checker": {
        "type": "boolean",
        "description": "If set to true, the safety checker will be enabled.",
        "required": false,
        "default": true
      },
      "output_format": {
        "type": "enum",
        "description": "The format of the generated image.",
        "required": false,
        "default": "jpeg",
        "enum": [
          "jpeg",
          "png"
        ]
      }
    },
    "capabilities": [
      "inference",
      "commercial use"
    ],
    "tags": [
      "editing"
    ],
    "lastUpdated": 1750565491597
  },
  "fal-ai/image2svg": {
    "id": "fal-ai/image2svg",
    "name": "Image2svg",
    "category": "image-to-image",
    "description": "Image2SVG transforms raster images into clean vector graphics, preserving visual quality while enabling scalable, customizable SVG outputs with precise control over detail levels.",
    "parameters": {
      "image_url": {
        "type": "string",
        "description": "The image to convert to SVG",
        "required": true
      },
      "colormode": {
        "type": "string",
        "description": "Choose between color or binary (black and white) output",
        "required": false,
        "default": "color",
        "enum": [
          "color",
          "binary"
        ]
      },
      "hierarchical": {
        "type": "string",
        "description": "Hierarchical mode: stacked or cutout",
        "required": false,
        "default": "stacked",
        "enum": [
          "stacked",
          "cutout"
        ]
      },
      "mode": {
        "type": "string",
        "description": "Mode: spline (curved) or polygon (straight lines)",
        "required": false,
        "default": "spline",
        "enum": [
          "spline",
          "polygon"
        ]
      },
      "filter_speckle": {
        "type": "integer",
        "description": "Filter out small speckles and noise",
        "required": false,
        "default": 4,
        "min": 0,
        "max": 20
      },
      "color_precision": {
        "type": "integer",
        "description": "Color quantization level",
        "required": false,
        "default": 6,
        "min": 1,
        "max": 10
      },
      "layer_difference": {
        "type": "integer",
        "description": "Layer difference threshold for hierarchical mode",
        "required": false,
        "default": 16,
        "min": 1,
        "max": 32
      },
      "corner_threshold": {
        "type": "integer",
        "description": "Corner detection threshold in degrees",
        "required": false,
        "default": 60,
        "min": 0,
        "max": 180
      },
      "length_threshold": {
        "type": "number",
        "description": "Length threshold for curves/lines",
        "required": false,
        "default": 4,
        "min": 0,
        "max": 10
      },
      "max_iterations": {
        "type": "integer",
        "description": "Maximum number of iterations for optimization",
        "required": false,
        "default": 10,
        "min": 1,
        "max": 20
      },
      "splice_threshold": {
        "type": "integer",
        "description": "Splice threshold for joining paths",
        "required": false,
        "default": 45,
        "min": 0,
        "max": 90
      },
      "path_precision": {
        "type": "integer",
        "description": "Decimal precision for path coordinates",
        "required": false,
        "default": 3,
        "min": 1,
        "max": 10
      }
    },
    "capabilities": [
      "inference",
      "commercial use"
    ],
    "tags": [
      "utility",
      "editing"
    ],
    "lastUpdated": 1750565493268
  },
  "fal-ai/magi": {
    "id": "fal-ai/magi",
    "name": "MAGI-1",
    "category": "text-to-video",
    "description": "MAGI-1 is a video generation model with exceptional understanding of physical interactions and cinematic prompts",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "The text prompt to guide video generation.",
        "required": true
      },
      "num_frames": {
        "type": "number",
        "description": "Number of frames to generate. Must be between 96 and 192 (inclusive). Each additional 24 frames beyond 96 incurs an additional billing unit.",
        "required": false,
        "default": 96,
        "min": 96,
        "max": 192
      },
      "seed": {
        "type": "number",
        "description": "Random seed for reproducibility. If None, a random seed is chosen.",
        "required": false
      },
      "resolution": {
        "type": "string",
        "description": "Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit.",
        "required": false,
        "default": "720p",
        "enum": [
          "480p",
          "720p"
        ]
      },
      "num_inference_steps": {
        "type": "number",
        "description": "Number of inference steps for sampling. Higher values give better quality but take longer.",
        "required": false,
        "default": 16,
        "enum": [
          4,
          8,
          16,
          32,
          64
        ]
      },
      "enable_safety_checker": {
        "type": "boolean",
        "description": "If set to true, the safety checker will be enabled.",
        "required": false,
        "default": true
      },
      "aspect_ratio": {
        "type": "string",
        "description": "Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image.",
        "required": false,
        "default": "auto",
        "enum": [
          "auto",
          "16:9",
          "9:16",
          "1:1"
        ]
      }
    },
    "capabilities": [
      "text-to-video"
    ],
    "tags": [
      "text-to-video"
    ],
    "lastUpdated": 1750565527466
  },
  "fal-ai/dia-tts": {
    "id": "fal-ai/dia-tts",
    "name": "Dia",
    "category": "text-to-speech",
    "description": "Dia directly generates realistic dialogue from transcripts. Audio conditioning enables emotion control. Produces natural nonverbals like laughter and throat clearing.",
    "parameters": {
      "text": {
        "type": "string",
        "description": "The text to be converted to speech.",
        "required": true
      },
      "ref_audio_url": {
        "type": "string",
        "description": "The URL of the reference audio file.",
        "required": true
      },
      "ref_text": {
        "type": "string",
        "description": "The reference text to be used for TTS.",
        "required": true
      }
    },
    "capabilities": [
      "text-to-speech",
      "emotion control",
      "natural nonverbals"
    ],
    "tags": [
      "text-to-speech"
    ],
    "lastUpdated": 1750565529759
  },
  "fal-ai/magi-distilled": {
    "id": "fal-ai/magi-distilled",
    "name": "MAGI-1 (Distilled)",
    "category": "text-to-video",
    "description": "MAGI-1 distilled is a faster video generation model with exceptional understanding of physical interactions and cinematic prompts",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "The text prompt to guide video generation.",
        "required": true
      },
      "num_frames": {
        "type": "number",
        "description": "Number of frames to generate. Must be between 96 and 192 (inclusive). Each additional 24 frames beyond 96 incurs an additional billing unit.",
        "required": false,
        "default": 96,
        "min": 96,
        "max": 192
      },
      "resolution": {
        "type": "string",
        "description": "Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit.",
        "required": false,
        "default": "720p",
        "enum": [
          "480p",
          "720p"
        ]
      },
      "num_inference_steps": {
        "type": "number",
        "description": "Number of inference steps for sampling. Higher values give better quality but take longer.",
        "required": false,
        "default": 16,
        "enum": [
          4,
          8,
          16,
          32
        ]
      },
      "enable_safety_checker": {
        "type": "boolean",
        "description": "If set to true, the safety checker will be enabled.",
        "required": false,
        "default": true
      },
      "aspect_ratio": {
        "type": "string",
        "description": "Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image.",
        "required": false,
        "default": "auto",
        "enum": [
          "auto",
          "16:9",
          "9:16",
          "1:1"
        ]
      }
    },
    "capabilities": [
      "text-to-video"
    ],
    "tags": [
      "text-to-video"
    ],
    "lastUpdated": 1750565532209
  },
  "fal-ai/uno": {
    "id": "fal-ai/uno",
    "name": "Uno",
    "category": "image-to-image",
    "description": "An AI model that transforms input images into new ones based on text prompts, blending reference visuals with your creative directions.",
    "parameters": {
      "input_image_urls": {
        "type": "array",
        "description": "URL of images to use while generating the image.",
        "required": true,
        "default": null
      },
      "image_size": {
        "type": "string",
        "description": "The size of the generated image. You can choose between some presets or custom height and width that must be multiples of 8.",
        "required": false,
        "default": "square_hd",
        "enum": [
          "square_hd",
          "square",
          "portrait_4_3",
          "portrait_16_9",
          "landscape_4_3",
          "landscape_16_9"
        ]
      },
      "prompt": {
        "type": "string",
        "description": "The prompt to generate an image from.",
        "required": true,
        "default": null
      },
      "seed": {
        "type": "number",
        "description": "Random seed for reproducible generation. If set none, a random seed will be used.",
        "required": false,
        "default": null
      },
      "num_images": {
        "type": "number",
        "description": "The number of images to generate.",
        "required": false,
        "default": 1,
        "min": 1,
        "max": 4
      },
      "num_inference_steps": {
        "type": "number",
        "description": "The number of inference steps to perform.",
        "required": false,
        "default": 28,
        "min": 1,
        "max": 50
      },
      "guidance_scale": {
        "type": "number",
        "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.",
        "required": false,
        "default": 3.5,
        "min": 1,
        "max": 20
      },
      "output_format": {
        "type": "string",
        "description": "The format of the generated image.",
        "required": false,
        "default": "jpeg",
        "enum": [
          "jpeg",
          "png"
        ]
      },
      "enable_safety_checker": {
        "type": "boolean",
        "description": "If set to true, the safety checker will be enabled.",
        "required": false,
        "default": true
      },
      "sync_mode": {
        "type": "boolean",
        "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.",
        "required": false,
        "default": false
      }
    },
    "capabilities": [
      "inference",
      "commercial use"
    ],
    "tags": [
      "image-to-image"
    ],
    "lastUpdated": 1750565537393
  },
  "fal-ai/plushify": {
    "id": "fal-ai/plushify",
    "name": "Plushify",
    "category": "image-to-image",
    "description": "Turn any image into a cute plushie!",
    "parameters": {
      "image_url": {
        "type": "string",
        "description": "URL of the image to apply cartoon style to",
        "required": true
      },
      "prompt": {
        "type": "string",
        "description": "Prompt for the generation. Default is empty which is usually best, but sometimes it can help to add a description of the subject.",
        "required": false,
        "default": ""
      },
      "scale": {
        "type": "float",
        "description": "Scale factor for the Cartoon effect",
        "required": false,
        "default": 1,
        "min": 0.1,
        "max": 2
      },
      "guidance_scale": {
        "type": "float",
        "description": "Guidance scale for the generation",
        "required": false,
        "default": 3.5,
        "min": 0,
        "max": 20
      },
      "num_inference_steps": {
        "type": "integer",
        "description": "Number of inference steps",
        "required": false,
        "default": 28,
        "min": 1,
        "max": 50
      },
      "enable_safety_checker": {
        "type": "boolean",
        "description": "Whether to enable the safety checker",
        "required": false,
        "default": true
      },
      "use_cfg_zero": {
        "type": "boolean",
        "description": "Whether to use CFG zero",
        "required": false,
        "default": false
      },
      "seed": {
        "type": "integer",
        "description": "The seed for image generation. Same seed with same parameters will generate same image.",
        "required": false
      },
      "num_images": {
        "type": "integer",
        "description": "Number of images to generate",
        "required": false,
        "default": 1,
        "min": 1,
        "max": 4
      }
    },
    "capabilities": [
      "inference"
    ],
    "tags": [],
    "lastUpdated": 1750565563114
  },
  "fal-ai/smart-turn": {
    "id": "fal-ai/smart-turn",
    "name": "Pipecat's Smart Turn model",
    "category": "speech-to-text",
    "description": "An open source, community-driven and native audio turn detection model by Pipecat AI.",
    "parameters": {
      "audio_url": {
        "type": "string",
        "description": "The URL of the audio file to be processed.",
        "required": true,
        "default": "https://fal.media/files/panda/5-QaAOC32rB_hqWaVdqEH.mpga"
      }
    },
    "capabilities": [
      "inference",
      "commercial use"
    ],
    "tags": [
      "audio",
      "turn detection",
      "speech-to-text"
    ],
    "lastUpdated": 1750565566619
  },
  "fal-ai/minimax-image": {
    "id": "fal-ai/minimax-image",
    "name": "MiniMax (Hailuo AI) Text to Image",
    "category": "text-to-image",
    "description": "Generate high quality images from text prompts using MiniMax Image-01. Longer text prompts will result in better quality images.",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "Text prompt for image generation (max 1500 characters)",
        "required": true,
        "default": null,
        "min": 1,
        "max": 1500
      },
      "aspect_ratio": {
        "type": "string",
        "description": "Aspect ratio of the generated image",
        "required": false,
        "default": "1:1",
        "enum": [
          "1:1",
          "16:9",
          "4:3",
          "3:2",
          "2:3",
          "3:4",
          "9:16",
          "21:9"
        ]
      },
      "num_images": {
        "type": "number",
        "description": "Number of images to generate (1-9)",
        "required": false,
        "default": 1,
        "min": 1,
        "max": 9
      },
      "prompt_optimizer": {
        "type": "boolean",
        "description": "Whether to enable automatic prompt optimization",
        "required": false,
        "default": false
      }
    },
    "capabilities": [
      "inference",
      "commercial use"
    ],
    "tags": [
      "stylized",
      "realism"
    ],
    "lastUpdated": 1750565569903
  },
  "fal-ai/instant-character": {
    "id": "fal-ai/instant-character",
    "name": "Instant Character",
    "category": "image-to-image",
    "description": "InstantCharacter creates high-quality, consistent characters from text prompts, supporting diverse poses, styles, and appearances with strong identity control.",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "The prompt to generate an image from.",
        "required": true
      },
      "image_url": {
        "type": "string",
        "description": "The image URL to generate an image from. Needs to match the dimensions of the mask.",
        "required": true
      },
      "image_size": {
        "type": "string",
        "description": "The size of the generated image. Default value: `square_hd`",
        "required": false,
        "default": "square_hd",
        "enum": [
          "square_hd",
          "square",
          "portrait_4_3",
          "portrait_16_9",
          "landscape_4_3",
          "landscape_16_9"
        ]
      },
      "scale": {
        "type": "number",
        "description": "The scale of the subject image. Higher values will make the subject image more prominent in the generated image. Default value: `1`",
        "required": false,
        "default": 1,
        "min": 0,
        "max": 2
      },
      "negative_prompt": {
        "type": "string",
        "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution). Default value: `\"\"`",
        "required": false,
        "default": ""
      },
      "guidance_scale": {
        "type": "number",
        "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`",
        "required": false,
        "default": 3.5,
        "min": 0,
        "max": 20
      },
      "num_inference_steps": {
        "type": "integer",
        "description": "The number of inference steps to perform. Default value: `28`",
        "required": false,
        "default": 28,
        "min": 1,
        "max": 50
      },
      "num_images": {
        "type": "integer",
        "description": "The number of images to generate. Default value: `1`",
        "required": false,
        "default": 1,
        "min": 1,
        "max": 4
      },
      "enable_safety_checker": {
        "type": "boolean",
        "description": "If set to true, the safety checker will be enabled. Default value: `true`",
        "required": false,
        "default": true
      },
      "output_format": {
        "type": "string",
        "description": "The format of the generated image. Default value: `\"jpeg\"`",
        "required": false,
        "default": "jpeg",
        "enum": [
          "jpeg",
          "png"
        ]
      }
    },
    "capabilities": [
      "image generation",
      "text-to-image",
      "image-to-image"
    ],
    "tags": [
      "personalization",
      "customization"
    ],
    "lastUpdated": 1750565579622
  },
  "fal-ai/recraft-creative-upscale": {
    "id": "fal-ai/recraft-creative-upscale",
    "name": "Recraft Creative Upscale (Deprecated)",
    "category": "image-to-image",
    "description": "Enhances a given raster image using 'crisp upscale' tool, increasing image resolution, making the image sharper and cleaner.",
    "parameters": {
      "image_url": {
        "type": "string",
        "description": "The URL of the image to be upscaled. Must be in PNG format.",
        "required": true,
        "default": null,
        "min": 1,
        "max": 2083
      },
      "sync_mode": {
        "type": "boolean",
        "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.",
        "required": false,
        "default": false
      },
      "enable_safety_checker": {
        "type": "boolean",
        "description": "If set to true, the safety checker will be enabled.",
        "required": false,
        "default": false
      }
    },
    "capabilities": [
      "inference",
      "commercial use"
    ],
    "tags": [
      "deprecated",
      "image upscaling"
    ],
    "lastUpdated": 1750565602734
  },
  "fal-ai/cartoonify": {
    "id": "fal-ai/cartoonify",
    "name": "Cartoonify",
    "category": "image-to-image",
    "description": "Transform images into 3D cartoon artwork using an AI model that applies cartoon stylization while preserving the original image's composition and details.",
    "parameters": {
      "image_url": {
        "type": "string",
        "description": "URL of the image to apply Pixar style to",
        "required": true
      },
      "scale": {
        "type": "number",
        "description": "Scale factor for the Pixar effect",
        "required": false,
        "default": 1,
        "min": 0.1,
        "max": 2
      },
      "guidance_scale": {
        "type": "number",
        "description": "Guidance scale for the generation",
        "required": false,
        "default": 3.5,
        "min": 0,
        "max": 20
      },
      "num_inference_steps": {
        "type": "integer",
        "description": "Number of inference steps",
        "required": false,
        "default": 28,
        "min": 1,
        "max": 50
      },
      "enable_safety_checker": {
        "type": "boolean",
        "description": "Whether to enable the safety checker",
        "required": false,
        "default": true
      },
      "use_cfg_zero": {
        "type": "boolean",
        "description": "Whether to use CFG zero",
        "required": false,
        "default": false
      },
      "seed": {
        "type": "integer",
        "description": "The seed for image generation. Same seed with same parameters will generate same image.",
        "required": false
      }
    },
    "capabilities": [
      "inference"
    ],
    "tags": [
      "stylized",
      "transform"
    ],
    "lastUpdated": 1750565612368
  },
  "fal-ai/speech-to-text": {
    "id": "fal-ai/speech-to-text",
    "name": "Speech-to-Text",
    "category": "speech-to-text",
    "description": "Leverage the rapid processing capabilities of AI models to enable accurate and efficient real-time speech-to-text transcription.",
    "parameters": {
      "audio_url": {
        "type": "string",
        "description": "Local filesystem path (or remote URL) to a long audio file",
        "required": true,
        "default": null
      },
      "use_pnc": {
        "type": "boolean",
        "description": "Whether to use Canary's built-in punctuation & capitalization",
        "required": false,
        "default": true
      }
    },
    "capabilities": [
      "Inference",
      "Commercial use",
      "Streaming"
    ],
    "tags": [],
    "lastUpdated": 1750565627588
  },
  "fal-ai/gemini-flash-edit": {
    "id": "fal-ai/gemini-flash-edit",
    "name": "Gemini Flash Edit Multi Image",
    "category": "image-to-image",
    "description": "Gemini Flash Edit is a model that can edit single image using a text prompt and a reference image.",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "The prompt for image generation or editing",
        "required": true,
        "min": 3,
        "max": 5000
      },
      "image_url": {
        "type": "string",
        "description": "Optional URL of an input image for editing. If not provided, generates a new image.",
        "required": true
      }
    },
    "capabilities": [
      "image editing",
      "text-to-image generation"
    ],
    "tags": [
      "editing"
    ],
    "lastUpdated": 1750565633932
  },
  "fal-ai/finegrain-eraser": {
    "id": "fal-ai/finegrain-eraser",
    "name": "finegrain eraser",
    "category": "image-to-image",
    "description": "Finegrain Eraser removes objects—along with their shadows, reflections, and lighting artifacts—using only natural language, seamlessly filling the scene with contextually accurate content.",
    "parameters": {
      "image_url": {
        "type": "string",
        "description": "URL of the image to edit",
        "required": true,
        "default": null
      },
      "prompt": {
        "type": "string",
        "description": "Text description of what to erase",
        "required": true,
        "default": null
      },
      "mode": {
        "type": "string",
        "description": "Erase quality mode",
        "required": false,
        "default": "standard",
        "enum": [
          "express",
          "standard",
          "premium"
        ]
      },
      "seed": {
        "type": "number",
        "description": "Random seed for reproducible generation",
        "required": false,
        "default": null,
        "min": 0,
        "max": 999
      }
    },
    "capabilities": [
      "object removal",
      "shadow removal",
      "reflection removal",
      "lighting artifact removal"
    ],
    "tags": [
      "utility",
      "editing"
    ],
    "lastUpdated": 1750565638511
  },
  "fal-ai/latentsync": {
    "id": "fal-ai/latentsync",
    "name": "LatentSync",
    "category": "video-to-video",
    "description": "LatentSync is a video-to-video model that generates lip sync animations from audio using advanced algorithms for high-quality synchronization.",
    "parameters": {
      "video_url": {
        "type": "string",
        "description": "The URL of the video to generate the lip sync for.",
        "required": true
      },
      "audio_url": {
        "type": "string",
        "description": "The URL of the audio to generate the lip sync for.",
        "required": true
      },
      "guidance_scale": {
        "type": "number",
        "description": "Guidance scale for the model inference.",
        "required": false,
        "default": 1,
        "min": 1,
        "max": 2
      },
      "seed": {
        "type": "integer",
        "description": "Random seed for generation. If None, a random seed will be used.",
        "required": false
      },
      "loop_mode": {
        "type": "enum",
        "description": "Video loop mode when audio is longer than video. Options: pingpong, loop",
        "required": false,
        "enum": [
          "pingpong",
          "loop"
        ]
      }
    },
    "capabilities": [
      "lip sync",
      "video-to-video"
    ],
    "tags": [
      "animation",
      "lip sync"
    ],
    "lastUpdated": 1750565651448
  },
  "fal-ai/eye-correct": {
    "id": "fal-ai/eye-correct",
    "name": "Eye Correct",
    "category": "video-to-video",
    "description": "Eye Correct is a video-to-video model that can correct eye direction in videos. It can be used to correct eye direction in videos.",
    "parameters": {
      "video_url": {
        "type": "string",
        "description": "The URL of the video to correct",
        "required": true
      }
    },
    "capabilities": [
      "inference",
      "commercial use"
    ],
    "tags": [
      "utility",
      "editing",
      "faces"
    ],
    "lastUpdated": 1750565677172
  },
  "fal-ai/wan-t2v": {
    "id": "fal-ai/wan-t2v",
    "name": "Wan-2.1 Text-to-Video",
    "category": "text-to-video",
    "description": "Wan-2.1 is a text-to-video model that generates high-quality videos with high visual quality and motion diversity from text prompts",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "The text prompt to guide video generation.",
        "required": true
      },
      "negative_prompt": {
        "type": "string",
        "description": "Negative prompt for video generation. Default value: 'bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards'",
        "required": false,
        "default": "bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards"
      },
      "num_frames": {
        "type": "integer",
        "description": "Number of frames to generate. Must be between 81 to 100 (inclusive).",
        "required": false,
        "default": 81,
        "min": 81,
        "max": 100
      },
      "frames_per_second": {
        "type": "integer",
        "description": "Frames per second of the generated video. Must be between 5 to 24.",
        "required": false,
        "default": 16,
        "min": 5,
        "max": 24
      },
      "seed": {
        "type": "integer",
        "description": "Random seed for reproducibility. If None, a random seed is chosen.",
        "required": false
      },
      "resolution": {
        "type": "string",
        "description": "Resolution of the generated video (480p, 580p, or 720p).",
        "required": false,
        "default": "720p",
        "enum": [
          "480p",
          "580p",
          "720p"
        ]
      },
      "aspect_ratio": {
        "type": "string",
        "description": "Aspect ratio of the generated video (16:9 or 9:16).",
        "required": false,
        "default": "16:9",
        "enum": [
          "9:16",
          "16:9"
        ]
      },
      "num_inference_st steps": {
        "type": "integer",
        "description": "Number of inference steps for sampling. Higher values give better quality but take longer.",
        "required": false,
        "default": 30,
        "min": 2,
        "max": 40
      },
      "enable_safety_checker": {
        "type": "boolean",
        "description": "If set to true, the safety checker will be enabled.",
        "required": false,
        "default": false
      },
      "enable_prompt_expansion": {
        "type": "boolean",
        "description": "Whether to enable prompt expansion.",
        "required": false,
        "default": false
      },
      "turbo_mode": {
        "type": "boolean",
        "description": "If true, the video will be generated faster with no noticeable degradation in the visual quality.",
        "required": false,
        "default": false
      }
    },
    "capabilities": [
      "text-to-video",
      "high-quality video generation",
      "motion diversity"
    ],
    "tags": [
      "text to video",
      "motion"
    ],
    "lastUpdated": 1750565688801
  },
  "fal-ai/mini-cpm": {
    "id": "fal-ai/mini-cpm",
    "name": "MiniCPM-V 2.6",
    "category": "vision",
    "description": "Multimodal vision-language model for single/multi image understanding",
    "parameters": {
      "image_urls": {
        "type": "array",
        "description": "List of image URLs to be used for the image description",
        "required": true,
        "default": null,
        "min": null,
        "max": null,
        "enum": null
      },
      "prompt": {
        "type": "string",
        "description": "Prompt to be used for the image description",
        "required": true,
        "default": null,
        "min": null,
        "max": null,
        "enum": null
      }
    },
    "capabilities": [
      "inference"
    ],
    "tags": [
      "multimodal",
      "vision"
    ],
    "lastUpdated": 1750565707343
  },
  "fal-ai/recraft-crisp-upscale": {
    "id": "fal-ai/recraft-crisp-upscale",
    "name": "Recraft Crisp Upscale",
    "category": "image-to-image",
    "description": "Enhances a given raster image using 'crisp upscale' tool, boosting resolution with a focus on refining small details and faces.",
    "parameters": {
      "image_url": {
        "type": "string",
        "description": "The URL of the image to be upscaled. Must be in PNG format.",
        "required": true,
        "default": null,
        "min": 1,
        "max": 2083
      },
      "sync_mode": {
        "type": "boolean",
        "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.",
        "required": false,
        "default": false
      },
      "enable_safety_checker": {
        "type": "boolean",
        "description": "If set to true, the safety checker will be enabled.",
        "required": false,
        "default": false
      }
    },
    "capabilities": [
      "inference"
    ],
    "tags": [
      "upscaling"
    ],
    "lastUpdated": 1750565716867
  },
  "fal-ai/omnigen-v1": {
    "id": "fal-ai/omnigen-v1",
    "name": "OmniGen v1",
    "category": "text-to-image",
    "description": "OmniGen is a unified image generation model that can generate a wide range of images from multi-modal prompts. It can be used for various tasks such as Image Editing, Personalized Image Generation, Virtual Try-On, Multi Person Generation and more!",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "The prompt to generate an image from.",
        "required": true,
        "default": ""
      },
      "input_image_urls": {
        "type": "array",
        "description": "URL of images to use while generating the image, Use <img><|image_1|></img> for the first image and so on.",
        "required": false,
        "default": []
      },
      "image_size": {
        "type": "string",
        "description": "The size of the generated image.",
        "required": false,
        "default": "square_hd",
        "enum": [
          "square_hd",
          "square",
          "portrait_4_3",
          "portrait_16_9",
          "landscape_4_3",
          "landscape_16_9"
        ]
      },
      "num_inference_steps": {
        "type": "number",
        "description": "The number of inference steps to perform.",
        "required": false,
        "default": 50,
        "min": 1,
        "max": 50
      },
      "guidance_scale": {
        "type": "number",
        "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.",
        "required": false,
        "default": 3,
        "min": 0,
        "max": 20
      },
      "img_guidance_scale": {
        "type": "number",
        "description": "The Image Guidance scale is a measure of how close you want the model to stick to your input image when looking for a related image to show you.",
        "required": false,
        "default": 1.6,
        "min": 0,
        "max": 20
      },
      "num_images": {
        "type": "number",
        "description": "The number of images to generate.",
        "required": false,
        "default": 1,
        "min": 1,
        "max": 4
      },
      "enable_safety_checker": {
        "type": "boolean",
        "description": "If set to true, the safety checker will be enabled.",
        "required": false,
        "default": true
      },
      "output_format": {
        "type": "string",
        "description": "The format of the generated image.",
        "required": false,
        "default": "jpeg",
        "enum": [
          "jpeg",
          "png"
        ]
      }
    },
    "capabilities": [
      "Image Editing",
      "Personalized Image Generation",
      "Virtual Try-On",
      "Multi Person Generation"
    ],
    "tags": [
      "multimodal",
      "editing",
      "try-on"
    ],
    "lastUpdated": 1750565718259
  },
  "fal-ai/cat-vton": {
    "id": "fal-ai/cat-vton",
    "name": "try-on",
    "category": "image-to-image",
    "description": "Image based high quality Virtual Try-On",
    "parameters": {
      "human_image_url": {
        "type": "string",
        "description": "Url for the human image.",
        "required": true,
        "default": ""
      },
      "garment_image_url": {
        "type": "string",
        "description": "Url to the garment image.",
        "required": true,
        "default": ""
      },
      "cloth_type": {
        "type": "string",
        "description": "Type of the Cloth to be tried on. Options: upper, lower, overall, inner, outer.",
        "required": true,
        "default": "upper",
        "enum": [
          "upper",
          "lower",
          "overall",
          "inner",
          "outer"
        ]
      },
      "image_size": {
        "type": "string",
        "description": "The size of the generated image.",
        "required": false,
        "default": "portrait_4_3",
        "enum": [
          "square_hd",
          "square",
          "portrait_4_3",
          "portrait_16_9",
          "landscape_4_3",
          "landscape_16_9"
        ]
      },
      "num_inference_steps": {
        "type": "integer",
        "description": "The number of inference steps to perform.",
        "required": false,
        "default": 30,
        "min": 1,
        "max": 50
      },
      "guidance_scale": {
        "type": "number",
        "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.",
        "required": false,
        "default": 2.5,
        "min": 0,
        "max": 20
      },
      "seed": {
        "type": "integer",
        "description": "The same seed and the same input given to the same version of the model will output the same image every time.",
        "required": false,
        "default": null
      }
    },
    "capabilities": [
      "inference"
    ],
    "tags": [
      "try-on",
      "fashion",
      "clothing"
    ],
    "lastUpdated": 1750565719816
  },
  "fal-ai/stable-video": {
    "id": "fal-ai/stable-video",
    "name": "High Quality Stable Video Diffusion",
    "category": "image-to-video",
    "description": "Generate short video clips from your images using SVD v1.1",
    "parameters": {
      "image_url": {
        "type": "string",
        "description": "The URL of the image to use as a starting point for the generation.",
        "required": true
      },
      "seed": {
        "type": "number",
        "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time.",
        "required": false
      },
      "motion_bucket_id": {
        "type": "number",
        "description": "The motion bucket id determines the motion of the generated video. The higher the number, the more motion there will be.",
        "required": false,
        "default": 127,
        "min": 1,
        "max": 255
      },
      "cond_aug": {
        "type": "number",
        "description": "The conditioning augmentation determines the amount of noise that will be added to the conditioning frame. The higher the number, the more noise there will be, and the less the video will look like the initial image.",
        "required": false,
        "default": 0.02,
        "min": 0,
        "max": 10
      },
      "fps": {
        "type": "number",
        "description": "The frames per second of the generated video.",
        "required": false,
        "default": 25,
        "min": 10,
        "max": 100
      }
    },
    "capabilities": [
      "inference"
    ],
    "tags": [],
    "lastUpdated": 1750565734605
  },
  "fal-ai/llavav15-13b": {
    "id": "fal-ai/llavav15-13b",
    "name": "LLaVA v1.5 13B",
    "category": "vision",
    "description": "Vision",
    "parameters": {
      "image_url": {
        "type": "string",
        "description": "URL of the image to be processed",
        "required": true
      },
      "prompt": {
        "type": "string",
        "description": "Prompt to be used for the image",
        "required": true
      },
      "max_tokens": {
        "type": "integer",
        "description": "Maximum number of tokens to generate",
        "required": false,
        "default": 64,
        "min": 32,
        "max": 1024
      },
      "temperature": {
        "type": "float",
        "description": "Temperature for sampling",
        "required": false,
        "default": 0.2,
        "max": 1
      },
      "top_p": {
        "type": "float",
        "description": "Top P for sampling",
        "required": false,
        "default": 1,
        "min": 0,
        "max": 1
      }
    },
    "capabilities": [
      "inference"
    ],
    "tags": [
      "multimodal",
      "vision"
    ],
    "lastUpdated": 1750565742662
  },
  "fal-ai/stable-diffusion-v3-medium": {
    "id": "fal-ai/stable-diffusion-v3-medium",
    "name": "Stable Diffusion V3",
    "category": "text-to-image",
    "description": "Stable Diffusion 3 Medium (Text to Image) is a Multimodal Diffusion Transformer (MMDiT) model that improves image quality, typography, prompt understanding, and efficiency.",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "The prompt to generate an image from.",
        "required": true
      },
      "negative_prompt": {
        "type": "string",
        "description": "The negative prompt to generate an image from.",
        "required": false,
        "default": ""
      },
      "prompt_expansion": {
        "type": "boolean",
        "description": "If set to true, prompt will be upsampled with more details.",
        "required": false,
        "default": false
      },
      "image_size": {
        "type": "string",
        "description": "The size of the generated image.",
        "required": false,
        "default": "square_hd",
        "enum": [
          "square_hd",
          "square",
          "portrait_4_3",
          "portrait_16_9",
          "landscape_4_3",
          "landscape_16_9"
        ]
      },
      "num_inference_steps": {
        "type": "integer",
        "description": "The number of inference steps to perform.",
        "required": false,
        "default": 28,
        "min": 1,
        "max": 50
      },
      "seed": {
        "type": "integer",
        "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time.",
        "required": false
      },
      "guidance_scale": {
        "type": "float",
        "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.",
        "required": false,
        "default": 5,
        "min": 0,
        "max": 20
      },
      "sync_mode": {
        "type": "boolean",
        "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response.",
        "required": false,
        "default": false
      },
      "num_images": {
        "type": "integer",
        "description": "The number of images to generate.",
        "required": false,
        "default": 1,
        "min": 1,
        "max": 4
      },
      "enable_safety_checker": {
        "type": "boolean",
        "description": "If set to true, the safety checker will be enabled.",
        "required": false,
        "default": true
      }
    },
    "capabilities": [
      "text-to-image"
    ],
    "tags": [
      "diffusion",
      "style"
    ],
    "lastUpdated": 1750565752004
  },
  "fal-ai/stable-cascade": {
    "id": "fal-ai/stable-cascade",
    "name": "Stable Cascade",
    "category": "text-to-image",
    "description": "Stable Cascade: Image generation on a smaller & cheaper latent space.",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "The prompt to use for generating the image. Be as descriptive as possible for best results.",
        "required": true
      },
      "negative_prompt": {
        "type": "string",
        "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution).",
        "required": false,
        "default": ""
      },
      "first_stage_steps": {
        "type": "number",
        "description": "Number of steps to run the first stage for.",
        "required": false,
        "default": 20,
        "min": 4,
        "max": 40
      },
      "second_stage_steps": {
        "type": "number",
        "description": "Number of steps to run the second stage for.",
        "required": false,
        "default": 10,
        "min": 4,
        "max": 24
      },
      "guidance_scale": {
        "type": "number",
        "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.",
        "required": false,
        "default": 4,
        "min": 0,
        "max": 20
      },
      "second_stage_guidance_scale": {
        "type": "number",
        "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.",
        "required": false,
        "default": 0,
        "min": 0,
        "max": 20
      },
      "image_size": {
        "type": "string",
        "description": "The size of the generated image.",
        "required": false,
        "default": "square_hd",
        "enum": [
          "square_hd",
          "square",
          "portrait_4_3",
          "portrait_16_9",
          "landscape_4_3",
          "landscape_16_9"
        ]
      },
      "seed": {
        "type": "number",
        "description": "The same seed and the same prompt given to the same version of Stable Cascade will output the same image every time.",
        "required": false
      },
      "enable_safety_checker": {
        "type": "boolean",
        "description": "If set to false, the safety checker will be disabled.",
        "required": false,
        "default": true
      },
      "num_images": {
        "type": "number",
        "description": "The number of images to generate.",
        "required": false,
        "default": 1,
        "min": 1,
        "max": 4
      },
      "sync_mode": {
        "type": "boolean",
        "description": "If set to true, the image will be returned as base64 encoded string.",
        "required": false,
        "default": false
      }
    },
    "capabilities": [
      "text-to-image"
    ],
    "tags": [
      "diffusion",
      "lcm"
    ],
    "lastUpdated": 1750565764164
  },
  "fal-ai/sdxl-controlnet-union": {
    "id": "fal-ai/sdxl-controlnet-union",
    "name": "SDXL ControlNet Union",
    "category": "text-to-image",
    "description": "An efficent SDXL multi-controlnet text-to-image model.",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "The prompt to use for generating the image. Be as descriptive as possible for best results.",
        "required": true
      },
      "controlnet_conditioning_scale": {
        "type": "number",
        "description": "The scale of the controlnet conditioning. Default value: `0.5`",
        "required": false,
        "default": 0.5,
        "min": 0,
        "max": 1
      },
      "negative_prompt": {
        "type": "string",
        "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution). Default value: `\"\"`",
        "required": false,
        "default": ""
      },
      "image_size": {
        "type": "union",
        "description": "The size of the generated image. Leave it none to automatically infer from the control image.",
        "required": false,
        "enum": [
          "square_hd",
          "square",
          "portrait_4_3",
          "portrait_16_9",
          "landscape_4_3",
          "landscape_16_9"
        ]
      },
      "num_inference_steps": {
        "type": "integer",
        "description": "The number of inference steps to perform. Default value: `35`",
        "required": false,
        "default": 35,
        "min": 1,
        "max": 70
      },
      "guidance_scale": {
        "type": "number",
        "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `7.5`",
        "required": false,
        "default": 7.5,
        "min": 0,
        "max": 20
      },
      "num_images": {
        "type": "integer",
        "description": "The number of images to generate. Default value: `1`",
        "required": false,
        "default": 1,
        "min": 1,
        "max": 8
      },
      "loras": {
        "type": "array",
        "description": "The list of LoRA weights to use. Default value: ``",
        "required": false,
        "default": []
      },
      "embeddings": {
        "type": "array",
        "description": "The list of embeddings to use. Default value: ``",
        "required": false,
        "default": []
      },
      "enable_safety_checker": {
        "type": "boolean",
        "description": "If set to true, the safety checker will be enabled. Default value: `true`",
        "required": false,
        "default": true
      },
      "safety_checker_version": {
        "type": "enum",
        "description": "The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model. Default value: `\"v1\"`",
        "required": false,
        "default": "v1",
        "enum": [
          "v1",
          "v2"
        ]
      },
      "format": {
        "type": "enum",
        "description": "The format of the generated image. Default value: `\"jpeg\"`",
        "required": false,
        "default": "jpeg",
        "enum": [
          "jpeg",
          "png"
        ]
      }
    },
    "capabilities": [
      "text-to-image",
      "image-to-image",
      "inpainting"
    ],
    "tags": [
      "diffusion",
      "controlnet",
      "composition"
    ],
    "lastUpdated": 1750565767005
  },
  "fal-ai/animatediff-v2v": {
    "id": "fal-ai/animatediff-v2v",
    "name": "AnimateDiff Video-to-Video Evolved",
    "category": "video-to-video",
    "description": "Re-animate your videos with evolved consistency!",
    "parameters": {
      "video_url": {
        "type": "string",
        "description": "URL of the video.",
        "required": true
      },
      "prompt": {
        "type": "string",
        "description": "The prompt to use for generating the image. Be as descriptive as possible for best results.",
        "required": true
      },
      "negative_prompt": {
        "type": "string",
        "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution).",
        "required": false,
        "default": "(bad quality, worst quality:1.2), ugly faces, bad anime"
      },
      "num_inference_steps": {
        "type": "number",
        "description": "Increasing the amount of steps tells Stable Diffusion that it should take more steps to generate your final result which can increase the amount of detail in your image.",
        "required": false,
        "default": 25,
        "min": 10,
        "max": 40
      },
      "guidance_scale": {
        "type": "number",
        "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.",
        "required": false,
        "default": 7,
        "min": 0,
        "max": 20
      },
      "base_model": {
        "type": "string",
        "description": "Base model to use for animation generation.",
        "required": false,
        "default": "cardosAnimev20",
        "enum": [
          "darkSushiMixMix_colorful",
          "cardosAnimev20"
        ]
      },
      "loras": {
        "type": "array",
        "description": "The list of LoRA weights to use.",
        "required": false,
        "default": []
      },
      "select_every_nth_frame": {
        "type": "number",
        "description": "Select every Nth frame from the video. This can be used to reduce the number of frames to process, which can reduce the time and the cost. However, it can also reduce the quality of the final video.",
        "required": false,
        "default": 2,
        "min": 1
      },
      "seed": {
        "type": "number",
        "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time.",
        "required": false
      }
    },
    "capabilities": [
      "inference",
      "commercial use"
    ],
    "tags": [
      "animation",
      "stylized"
    ],
    "lastUpdated": 1750565848190
  },
  "fal-ai/playground-v25": {
    "id": "fal-ai/playground-v25",
    "name": "Playground v2.5",
    "category": "text-to-image",
    "description": "State-of-the-art open-source model in aesthetic quality",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "The prompt to use for generating the image. Be as descriptive as possible for best results.",
        "required": true
      },
      "negative_prompt": {
        "type": "string",
        "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution).",
        "required": false,
        "default": ""
      },
      "image_size": {
        "type": "string",
        "description": "The size of the generated image.",
        "required": false,
        "default": "square_hd",
        "enum": [
          "square_hd",
          "square",
          "portrait_4_3",
          "portrait_16_9",
          "landscape_4_3",
          "landscape_16_9"
        ]
      },
      "num_inference_steps": {
        "type": "integer",
        "description": "The number of inference steps to perform.",
        "required": false,
        "default": 25,
        "min": 1,
        "max": 50
      },
      "guidance_scale": {
        "type": "float",
        "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.",
        "required": false,
        "default": 3,
        "min": 0,
        "max": 20
      },
      "num_images": {
        "type": "integer",
        "description": "The number of images to generate.",
        "required": false,
        "default": 1,
        "min": 1,
        "max": 8
      },
      "embeddings": {
        "type": "array",
        "description": "The list of embeddings to use.",
        "required": false,
        "default": []
      },
      "enable_safety_checker": {
        "type": "boolean",
        "description": "If set to true, the safety checker will be enabled.",
        "required": false,
        "default": true
      },
      "safety_checker_version": {
        "type": "string",
        "description": "The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model.",
        "required": false,
        "default": "v1",
        "enum": [
          "v1",
          "v2"
        ]
      },
      "format": {
        "type": "string",
        "description": "The format of the generated image.",
        "required": false,
        "default": "jpeg",
        "enum": [
          "jpeg",
          "png"
        ]
      },
      "guidance_rescale": {
        "type": "float",
        "description": "The rescale factor for the CFG.",
        "required": false,
        "default": 0,
        "min": 0,
        "max": 1
      }
    },
    "capabilities": [
      "text-to-image",
      "image-to-image",
      "inpainting"
    ],
    "tags": [
      "artistic",
      "style"
    ],
    "lastUpdated": 1750565850050
  },
  "fal-ai/photomaker": {
    "id": "fal-ai/photomaker",
    "name": "PhotoMaker",
    "category": "image-to-image",
    "description": "Customizing Realistic Human Photos via Stacked ID Embedding",
    "parameters": {
      "image_archive_url": {
        "type": "string",
        "description": "The URL of the image archive containing the images you want to use.",
        "required": true
      },
      "prompt": {
        "type": "string",
        "description": "The prompt to use for generating the image. Be as descriptive as possible for best results.",
        "required": true
      },
      "base_pipeline": {
        "type": "string",
        "description": "The base pipeline to use for generating the image.",
        "required": false,
        "default": "photomaker",
        "enum": [
          "photomaker",
          "photomaker-style"
        ]
      },
      "initial_image_url": {
        "type": "string",
        "description": "Optional initial image for img2img",
        "required": false
      },
      "initial_image_strength": {
        "type": "number",
        "description": "How much noise to add to the latent image. O for no noise, 1 for maximum noise.",
        "required": false,
        "default": 0.5,
        "min": 0,
        "max": 1
      },
      "style": {
        "type": "string",
        "description": "The style to apply to the generated image.",
        "required": false,
        "default": "Photographic",
        "enum": [
          "(No style)",
          "Cinematic",
          "Disney Character",
          "Digital Art",
          "Photographic",
          "Fantasy art",
          "Neonpunk",
          "Enhance",
          "Comic book",
          "Lowpoly",
          "Line art"
        ]
      },
      "negative_prompt": {
        "type": "string",
        "description": "The negative prompt to use. Use it to address details that you don't want in the image.",
        "required": false,
        "default": ""
      },
      "num_inference_steps": {
        "type": "integer",
        "description": "Increasing the amount of steps tells Stable Diffusion that it should take more steps to generate your final result which can increase the amount of detail in your image.",
        "required": false,
        "default": 50,
        "min": 20,
        "max": 100
      },
      "style_strength": {
        "type": "integer",
        "description": "The strength of the style to apply to the generated image.",
        "required": false,
        "default": 20,
        "min": 15,
        "max": 50
      },
      "num_images": {
        "type": "integer",
        "description": "Number of images to generate in one request. Note that the higher the batch size, the longer it will take to generate the images.",
        "required": false,
        "default": 1,
        "min": 1,
        "max": 4
      },
      "guidance_scale": {
        "type": "number",
        "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.",
        "required": false,
        "default": 5,
        "min": 0.1,
        "max": 10
      },
      "seed": {
        "type": "integer",
        "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time.",
        "required": false
      }
    },
    "capabilities": [
      "inference",
      "commercial use"
    ],
    "tags": [
      "editing",
      "customization",
      "realism",
      "personalization"
    ],
    "lastUpdated": 1750565851959
  },
  "fal-ai/fast-lightning-sdxl": {
    "id": "fal-ai/fast-lightning-sdxl",
    "name": "Stable Diffusion XL Lightning",
    "category": "text-to-image",
    "description": "Run SDXL at the speed of light",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "The prompt to use for generating the image. Be as descriptive as possible for best results.",
        "required": true
      },
      "image_size": {
        "type": "string",
        "description": "The size of the generated image. Default value: `square_hd`",
        "required": false,
        "default": "square_hd",
        "enum": [
          "square_hd",
          "square",
          "portrait_4_3",
          "portrait_16_9",
          "landscape_4_3",
          "landscape_16_9"
        ]
      },
      "num_inference_steps": {
        "type": "string",
        "description": "The number of inference steps to perform. Default value: `4`",
        "required": false,
        "default": "4",
        "enum": [
          "1",
          "2",
          "4",
          "8"
        ]
      },
      "seed": {
        "type": "integer",
        "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time.",
        "required": false
      },
      "sync_mode": {
        "type": "boolean",
        "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.",
        "required": false,
        "default": false
      },
      "num_images": {
        "type": "integer",
        "description": "The number of images to generate. Default value: `1`",
        "required": false,
        "default": 1,
        "min": 1,
        "max": 8
      },
      "embeddings": {
        "type": "array",
        "description": "The list of embeddings to use. Default value: ``",
        "required": false,
        "default": []
      },
      "enable_safety_checker": {
        "type": "boolean",
        "description": "If set to true, the safety checker will be enabled. Default value: `true`",
        "required": false,
        "default": true
      },
      "safety_checker_version": {
        "type": "string",
        "description": "The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model. Default value: `v1`",
        "required": false,
        "default": "v1",
        "enum": [
          "v1",
          "v2"
        ]
      },
      "expand_prompt": {
        "type": "boolean",
        "description": "If set to true, the prompt will be expanded with additional prompts.",
        "required": false,
        "default": false
      },
      "format": {
        "type": "string",
        "description": "The format of the generated image. Default value: `jpeg`",
        "required": false,
        "default": "jpeg",
        "enum": [
          "jpeg",
          "png"
        ]
      },
      "guidance_rescale": {
        "type": "float",
        "description": "The rescale factor for the CFG.",
        "required": false,
        "default": 0,
        "min": 0,
        "max": 1
      },
      "request_id": {
        "type": "string",
        "description": "An id bound to a request, can be used with response to identify the request itself. Default value: ``",
        "required": false,
        "default": ""
      }
    },
    "capabilities": [
      "text-to-image",
      "real-time"
    ],
    "tags": [
      "diffusion",
      "lightning",
      "real-time"
    ],
    "lastUpdated": 1750565853616
  },
  "fal-ai/esrgan": {
    "id": "fal-ai/esrgan",
    "name": "Upscale Images",
    "category": "image-to-image",
    "description": "Upscale images by a given factor.",
    "parameters": {
      "image_url": {
        "type": "string",
        "description": "Url to input image",
        "required": true,
        "default": ""
      },
      "scale": {
        "type": "number",
        "description": "Rescaling factor",
        "required": false,
        "default": 2,
        "min": 1,
        "max": 8
      },
      "model": {
        "type": "string",
        "description": "Model to use for upscaling",
        "required": false,
        "default": "RealESRGAN_x4plus",
        "enum": [
          "RealESRGAN_x4plus",
          "RealESRGAN_x2plus",
          "RealESRGAN_x4plus_anime_6B",
          "RealESRGAN_x4_v3",
          "RealESRGAN_x4_wdn_v3",
          "RealESRGAN_x4_anime_v3"
        ]
      },
      "output_format": {
        "type": "string",
        "description": "Output image format (png or jpeg)",
        "required": false,
        "default": "png",
        "enum": [
          "png",
          "jpeg"
        ]
      },
      "face": {
        "type": "boolean",
        "description": "Upscaling a face",
        "required": false,
        "default": false
      },
      "tile": {
        "type": "integer",
        "description": "Tile size. Default is 0, that is no tile. When encountering the out-of-GPU-memory issue, please specify it, e.g., 400 or 200",
        "required": false,
        "default": 0
      }
    },
    "capabilities": [
      "inference"
    ],
    "tags": [
      "upscaling",
      "high-res"
    ],
    "lastUpdated": 1750565923111
  },
  "fal-ai/sana": {
    "id": "fal-ai/sana",
    "name": "Sana",
    "category": "text-to-image",
    "description": "Sana can synthesize high-resolution, high-quality images with strong text-image alignment at a remarkably fast speed, with the ability to generate 4K images in less than a second.",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "The prompt to generate an image from.",
        "required": true
      },
      "negative_prompt": {
        "type": "string",
        "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution).",
        "required": false,
        "default": ""
      },
      "image_size": {
        "type": "union",
        "description": "The size of the generated image.",
        "required": false,
        "default": {
          "height": 2160,
          "width": 3840
        }
      },
      "num_inference_steps": {
        "type": "integer",
        "description": "The number of inference steps to perform.",
        "required": false,
        "default": 18,
        "min": 1,
        "max": 50
      },
      "seed": {
        "type": "integer",
        "description": "The same seed and the same prompt given to the same version of the model will output the same image every time.",
        "required": false
      },
      "guidance_scale": {
        "type": "float",
        "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.",
        "required": false,
        "default": 5,
        "min": 0,
        "max": 20
      },
      "sync_mode": {
        "type": "boolean",
        "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.",
        "required": false,
        "default": false
      },
      "num_images": {
        "type": "integer",
        "description": "The number of images to generate.",
        "required": false,
        "default": 1,
        "min": 1,
        "max": 4
      },
      "enable_safety_checker": {
        "type": "boolean",
        "description": "If set to true, the safety checker will be enabled.",
        "required": false,
        "default": true
      },
      "output_format": {
        "type": "enum",
        "description": "The format of the generated image.",
        "required": false,
        "default": "jpeg",
        "enum": [
          "jpeg",
          "png"
        ]
      },
      "style_name": {
        "type": "enum",
        "description": "The style to generate the image in.",
        "required": false,
        "default": "(No style)",
        "enum": [
          "(No style)",
          "Cinematic",
          "Photographic",
          "Anime",
          "Manga",
          "Digital Art",
          "Pixel art",
          "Fantasy art",
          "Neonpunk",
          "3D Model"
        ]
      }
    },
    "capabilities": [
      "text-to-image",
      "4k",
      "high-speed"
    ],
    "tags": [
      "text-to-image",
      "4k",
      "high-speed"
    ],
    "lastUpdated": 1750565934328
  },
  "fal-ai/orpheus-tts": {
    "id": "fal-ai/orpheus-tts",
    "name": "Orpheus TTS",
    "category": "text-to-speech",
    "description": "Orpheus TTS is a state-of-the-art, Llama-based Speech-LLM designed for high-quality, empathetic text-to-speech generation. This model has been finetuned to deliver human-level speech synthesis, achieving exceptional clarity, expressiveness, and real-time performances.",
    "parameters": {
      "text": {
        "type": "string",
        "description": "The text to be converted to speech. You can additionally add the following emotive tags: <laugh>, <chuckle>, <sigh>, <cough>, <sniffle>, <groan>, <yawn>, <gasp>",
        "required": true
      },
      "voice": {
        "type": "enum",
        "description": "Voice ID for the desired voice. Default value: 'tara'",
        "required": false,
        "default": "tara",
        "enum": [
          "tara",
          "leah",
          "jess",
          "leo",
          "dan",
          "mia",
          "zac",
          "zoe"
        ]
      },
      "temperature": {
        "type": "float",
        "description": "Temperature for generation (higher = more creative). Default value: 0.7",
        "required": false,
        "default": 0.7,
        "min": 0,
        "max": 2
      },
      "repetition_penalty": {
        "type": "float",
        "description": "Repetition penalty (>= 1.1 required for stable generations). Default value: 1.2",
        "required": false,
        "default": 1.2,
        "min": 1.1,
        "max": 2
      }
    },
    "capabilities": [
      "text-to-speech",
      "voice synthesis",
      "high-fidelity"
    ],
    "tags": [
      "text to speech",
      "voice synthesis",
      "high-fidelity"
    ],
    "lastUpdated": 1750565934411
  },
  "fal-ai/whisper": {
    "id": "fal-ai/whisper",
    "name": "Whisper",
    "category": "speech-to-text",
    "description": "Whisper is a model for speech transcription and translation.",
    "parameters": {
      "audio_url": {
        "type": "string",
        "description": "URL of the audio file to transcribe. Supported formats: mp3, mp4, mpeg, mpga, m4a, wav or webm.",
        "required": true,
        "default": "https://storage.googleapis.com/falserverless/model_tests/whisper/dinner_conversation.mp3"
      },
      "task": {
        "type": "string",
        "description": "Task to perform on the audio file. Either transcribe or translate.",
        "required": false,
        "default": "transcribe",
        "enum": [
          "transcribe",
          "translate"
        ]
      },
      "language": {
        "type": "string",
        "description": "Language of the audio file. If set to null, the language will be automatically detected. Defaults to null. If translate is selected as the task, the audio will be translated to English, regardless of the language selected.",
        "required": false,
        "default": null,
        "enum": [
          "af",
          "am",
          "ar",
          "as",
          "az",
          "ba",
          "be",
          "bg",
          "bn",
          "bo",
          "br",
          "bs",
          "ca",
          "cs",
          "cy",
          "da",
          "de",
          "el",
          "en",
          "es",
          "et",
          "eu",
          "fa",
          "fi",
          "fo",
          "fr",
          "gl",
          "gu",
          "ha",
          "haw",
          "he",
          "hi",
          "hr",
          "ht",
          "hu",
          "hy",
          "id",
          "is",
          "it",
          "ja",
          "jw",
          "ka",
          "kk",
          "km",
          "kn",
          "ko",
          "la",
          "lb",
          "ln",
          "lo",
          "lt",
          "lv",
          "mg",
          "mi",
          "mk",
          "ml",
          "mn",
          "mr",
          "ms",
          "mt",
          "my",
          "ne",
          "nl",
          "nn",
          "no",
          "oc",
          "pa",
          "pl",
          "ps",
          "pt",
          "ro",
          "ru",
          "sa",
          "sd",
          "si",
          "sk",
          "sl",
          "sn",
          "so",
          "sq",
          "sr",
          "su",
          "sv",
          "sw",
          "ta",
          "te",
          "tg",
          "th",
          "tk",
          "tl",
          "tr",
          "tt",
          "uk",
          "ur",
          "uz",
          "vi",
          "yi",
          "yo",
          "yue",
          "zh"
        ]
      },
      "diarize": {
        "type": "boolean",
        "description": "Whether to diarize the audio file. Defaults to false. Setting to true will add costs proportional to diarization inference time.",
        "required": false,
        "default": false
      },
      "chunk_level": {
        "type": "string",
        "description": "Level of the chunks to return. Either segment or word.",
        "required": false,
        "default": "segment",
        "enum": [
          "segment",
          "word"
        ]
      },
      "version": {
        "type": "string",
        "description": "Version of the model to use. All of the models are the Whisper large variant.",
        "required": false,
        "default": "3",
        "enum": [
          "3"
        ]
      },
      "batch_size": {
        "type": "integer",
        "description": "Batch size for processing.",
        "required": false,
        "default": 64,
        "min": 1,
        "max": 64
      },
      "prompt": {
        "type": "string",
        "description": "Prompt to use for generation. Defaults to an empty string.",
        "required": false,
        "default": ""
      },
      "num_speakers": {
        "type": "integer",
        "description": "Number of speakers in the audio file. Defaults to null. If not provided, the number of speakers will be automatically detected.",
        "required": false,
        "default": null,
        "min": 1
      }
    },
    "capabilities": [
      "transcription",
      "translation",
      "speech"
    ],
    "tags": [
      "transcription",
      "translation",
      "speech"
    ],
    "lastUpdated": 1750565936913
  },
  "fal-ai/sync-lipsync": {
    "id": "fal-ai/sync-lipsync",
    "name": "sync.so -- lipsync 1.9.0-beta",
    "category": "video-to-video",
    "description": "Generate realistic lipsync animations from audio using advanced algorithms for high-quality synchronization.",
    "parameters": {
      "video_url": {
        "type": "string",
        "description": "URL of the input video",
        "required": true,
        "default": ""
      },
      "audio_url": {
        "type": "string",
        "description": "URL of the input audio",
        "required": true,
        "default": ""
      },
      "sync_mode": {
        "type": "string",
        "description": "Lipsync mode when audio and video durations are out of sync.",
        "required": false,
        "default": "cut_off",
        "enum": [
          "cut_off",
          "loop",
          "bounce",
          "silence",
          "remap"
        ]
      },
      "model": {
        "type": "string",
        "description": "The model to use for lipsyncing",
        "required": false,
        "default": "lipsync-1.9.0-beta",
        "enum": [
          "lipsync-1.8.0",
          "lipsync-1.7.1",
          "lipsync-1.9.0-beta"
        ]
      }
    },
    "capabilities": [
      "inference",
      "commercial use"
    ],
    "tags": [
      "animation",
      "lip sync"
    ],
    "lastUpdated": 1750565962945
  },
  "fal-ai/star-vector": {
    "id": "fal-ai/star-vector",
    "name": "StarVector",
    "category": "image-to-image",
    "description": "AI vectorization model that transforms raster images into scalable SVG graphics, preserving visual details while enabling infinite scaling and easy editing capabilities.",
    "parameters": {
      "image_url": {
        "type": "string",
        "description": "URL of image to be used for relighting",
        "required": true,
        "default": null,
        "min": null,
        "max": null,
        "enum": null
      },
      "seed": {
        "type": "integer",
        "description": "seed to be used for generation",
        "required": false,
        "default": null,
        "min": null,
        "max": null,
        "enum": null
      }
    },
    "capabilities": [
      "inference",
      "commercial use"
    ],
    "tags": [
      "image-to-image"
    ],
    "lastUpdated": 1750565975272
  },
  "fal-ai/ghiblify": {
    "id": "fal-ai/ghiblify",
    "name": "Ghiblify Images",
    "category": "image-to-image",
    "description": "Reimagine and transform your ordinary photos into enchanting Studio Ghibli style artwork",
    "parameters": {
      "image_url": {
        "type": "string",
        "description": "The URL of the image to upscale.",
        "required": true,
        "default": null
      },
      "seed": {
        "type": "integer",
        "description": "The seed to use for the upscale. If not provided, a random seed will be used.",
        "required": false,
        "default": null
      },
      "enable_safety_checker": {
        "type": "boolean",
        "description": "Whether to enable the safety checker.",
        "required": false,
        "default": true
      }
    },
    "capabilities": [
      "inference"
    ],
    "tags": [
      "stylized",
      "transform"
    ],
    "lastUpdated": 1750565975440
  },
  "fal-ai/hunyuan-video": {
    "id": "fal-ai/hunyuan-video",
    "name": "Hunyuan Video",
    "category": "text-to-video",
    "description": "Hunyuan Video is an Open video generation model with high visual quality, motion diversity, text-video alignment, and generation stability. This endpoint generates videos from text descriptions.",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "The prompt to generate the video from.",
        "required": true
      },
      "aspect_ratio": {
        "type": "string",
        "description": "The aspect ratio of the video to generate.",
        "required": false,
        "default": "16:9",
        "enum": [
          "16:9",
          "9:16"
        ]
      },
      "resolution": {
        "type": "string",
        "description": "The resolution of the video to generate.",
        "required": false,
        "default": "720p",
        "enum": [
          "480p",
          "580p",
          "720p"
        ]
      },
      "enable_safety_checker": {
        "type": "boolean",
        "description": "If set to true, the safety checker will be enabled.",
        "required": false,
        "default": false
      },
      "num_inference_steps": {
        "type": "integer",
        "description": "The number of inference steps to run. Lower gets faster results, higher gets better results.",
        "required": false,
        "default": 30,
        "min": 2,
        "max": 30
      },
      "seed": {
        "type": "integer",
        "description": "The seed to use for generating the video.",
        "required": false
      },
      "num_frames": {
        "type": "string",
        "description": "The number of frames to generate.",
        "required": false,
        "default": "129",
        "enum": [
          "129",
          "85"
        ]
      },
      "pro_mode": {
        "type": "boolean",
        "description": "By default, generations are done with 35 steps. Pro mode does 55 steps which results in higher quality videos but will take more time and cost 2x more billing units.",
        "required": false,
        "default": false
      }
    },
    "capabilities": [
      "text-to-video"
    ],
    "tags": [
      "motion"
    ],
    "lastUpdated": 1750565981086
  },
  "fal-ai/thera": {
    "id": "fal-ai/thera",
    "name": "Thera",
    "category": "image-to-image",
    "description": "Fix low resolution images with fast speed and quality of thera.",
    "parameters": {
      "image_url": {
        "type": "string",
        "description": "URL of image to be used for upscaling",
        "required": true,
        "default": null
      },
      "upscaling": {
        "type": "number",
        "description": "Upscaling factor",
        "required": false,
        "default": 2,
        "min": 1,
        "max": 6
      },
      "backbone": {
        "type": "string",
        "description": "Backbone to use for upscaling",
        "required": true,
        "default": "edsr",
        "enum": [
          "edsr",
          "rdn"
        ]
      },
      "seed": {
        "type": "integer",
        "description": "Random seed for reproducible generation",
        "required": false,
        "default": null
      }
    },
    "capabilities": [
      "image upscaling"
    ],
    "tags": [
      "image processing",
      "super-resolution"
    ],
    "lastUpdated": 1750566006524
  },
  "fal-ai/mix-dehaze-net": {
    "id": "fal-ai/mix-dehaze-net",
    "name": "MixDehazer",
    "category": "image-to-image",
    "description": "An advanced dehaze model to remove atmospheric haze, restoring clarity and detail in images through intelligent neural network processing.",
    "parameters": {
      "image_url": {
        "type": "string",
        "description": "URL of image to be used for image enhancement",
        "required": true
      },
      "model": {
        "type": "string",
        "description": "Model to be used for dehazing. Default value: 'indoor'",
        "required": false,
        "default": "indoor",
        "enum": [
          "indoor",
          "outdoor"
        ]
      },
      "seed": {
        "type": "integer",
        "description": "Seed to be used for generation",
        "required": false
      }
    },
    "capabilities": [
      "inference"
    ],
    "tags": [
      "image-to-image",
      "dehazing",
      "neural network"
    ],
    "lastUpdated": 1750566008124
  },
  "fal-ai/invisible-watermark": {
    "id": "fal-ai/invisible-watermark",
    "name": "Invisible Watermark",
    "category": "image-to-image",
    "description": "Invisible Watermark is a model that can add an invisible watermark to an image.",
    "parameters": {
      "image_url": {
        "type": "string",
        "description": "URL of image to be watermarked or decoded",
        "required": true,
        "default": "https://storage.googleapis.com/falserverless/web-examples/watermark/watermark_ex.png"
      },
      "watermark": {
        "type": "string",
        "description": "Text to use as watermark (for encoding only)",
        "required": false,
        "default": "watermark"
      },
      "decode": {
        "type": "boolean",
        "description": "Whether to decode a watermark from the image instead of encoding",
        "required": false,
        "default": false
      },
      "length": {
        "type": "integer",
        "description": "Length of watermark bits to decode (required when decode=True)",
        "required": false,
        "default": 0
      }
    },
    "capabilities": [
      "inference"
    ],
    "tags": [
      "utility",
      "editing"
    ],
    "lastUpdated": 1750566009968
  },
  "fal-ai/csm-1b": {
    "id": "fal-ai/csm-1b",
    "name": "CSM-1B",
    "category": "text-to-audio",
    "description": "CSM (Conversational Speech Model) is a speech generation model from Sesame that generates RVQ audio codes from text and audio inputs.",
    "parameters": {
      "scene": {
        "type": "array",
        "description": "The text to generate an audio from.",
        "required": true
      },
      "context": {
        "type": "array",
        "description": "The context to generate an audio from.",
        "required": false
      }
    },
    "capabilities": [
      "text-to-audio",
      "conversational"
    ],
    "tags": [
      "conversational",
      "text to speech"
    ],
    "lastUpdated": 1750566030717
  },
  "fal-ai/ltx-video-v095": {
    "id": "fal-ai/ltx-video-v095",
    "name": "LTX Video-0.9.5",
    "category": "text-to-video",
    "description": "Generate videos from prompts using LTX Video-0.9.5",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "Text prompt to guide generation",
        "required": true,
        "default": null
      },
      "negative_prompt": {
        "type": "string",
        "description": "Negative prompt for generation",
        "required": false,
        "default": "worst quality, inconsistent motion, blurry, jittery, distorted"
      },
      "resolution": {
        "type": "string",
        "description": "Resolution of the generated video (480p or 720p).",
        "required": false,
        "default": "720p",
        "enum": [
          "480p",
          "720p"
        ]
      },
      "aspect_ratio": {
        "type": "string",
        "description": "Aspect ratio of the generated video (16:9 or 9:16).",
        "required": false,
        "default": "16:9",
        "enum": [
          "9:16",
          "16:9"
        ]
      },
      "seed": {
        "type": "integer",
        "description": "Random seed for generation",
        "required": false,
        "default": null
      },
      "num_inference_steps": {
        "type": "integer",
        "description": "Number of inference steps",
        "required": false,
        "default": 40,
        "min": 2,
        "max": 50
      },
      "expand_prompt": {
        "type": "boolean",
        "description": "Whether to expand the prompt using the model's own capabilities.",
        "required": false,
        "default": true
      }
    },
    "capabilities": [
      "text-to-video"
    ],
    "tags": [
      "video",
      "text-video"
    ],
    "lastUpdated": 1750566037666
  },
  "fal-ai/hunyuan-video-image-to-video": {
    "id": "fal-ai/hunyuan-video-image-to-video",
    "name": "Hunyuan Video Image-to-Video Inference",
    "category": "image-to-video",
    "description": "Image to Video for the high-quality Hunyuan Video I2V model.",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "The prompt to generate the video from.",
        "required": true,
        "max": 1000
      },
      "image_url": {
        "type": "string",
        "description": "URL of the image input.",
        "required": true
      },
      "seed": {
        "type": "integer",
        "description": "The seed to use for generating the video.",
        "required": false
      },
      "aspect_ratio": {
        "type": "string",
        "description": "The aspect ratio of the video to generate.",
        "required": false,
        "default": "16:9",
        "enum": [
          "16:9",
          "9:16"
        ]
      },
      "resolution": {
        "type": "string",
        "description": "The resolution of the video to generate.",
        "required": false,
        "default": "720p",
        "enum": [
          "720p"
        ]
      },
      "num_frames": {
        "type": "string",
        "description": "The number of frames to generate.",
        "required": false,
        "default": "129",
        "enum": [
          "129"
        ]
      },
      "i2v_stability": {
        "type": "boolean",
        "description": "Turning on I2V Stability reduces hallucination but also reduces motion.",
        "required": false,
        "default": false
      }
    },
    "capabilities": [
      "inference",
      "commercial use"
    ],
    "tags": [
      "motion"
    ],
    "lastUpdated": 1750566047817
  },
  "fal-ai/swin2sr": {
    "id": "fal-ai/swin2sr",
    "name": "SWIN2SR",
    "category": "image-to-image",
    "description": "Enhance low-resolution images with the superior quality of Swin2SR for sharper, clearer results.",
    "parameters": {
      "image_url": {
        "type": "string",
        "description": "URL of image to be used for image enhancement",
        "required": true
      },
      "seed": {
        "type": "integer",
        "description": "seed to be used for generation",
        "required": false
      },
      "task": {
        "type": "enum",
        "description": "Task to perform Default value: \"classical_sr\"",
        "required": false,
        "default": "classical_sr",
        "enum": [
          "classical_sr",
          "compressed_sr",
          "real_sr"
        ]
      }
    },
    "capabilities": [
      "image-enhancement"
    ],
    "tags": [
      "image-enhancement"
    ],
    "lastUpdated": 1750566077832
  },
  "fal-ai/docres": {
    "id": "fal-ai/docres",
    "name": "DocRes",
    "category": "image-to-image",
    "description": "Enhance low-resolution, blur, shadowed documents with the superior quality of docres for sharper, clearer results.",
    "parameters": {
      "image_url": {
        "type": "string",
        "description": "URL of image to be used for relighting",
        "required": true,
        "default": "https://storage.googleapis.com/falserverless/docres_ckpt/218_in.png"
      },
      "task": {
        "type": "enum",
        "description": "Task to perform",
        "required": true,
        "default": "deshadowing",
        "enum": [
          "deshadowing",
          "appearance",
          "deblurring",
          "binarization"
        ]
      },
      "seed": {
        "type": "integer",
        "description": "The same seed and the same prompt given to the same version of the model will output the same image every time.",
        "required": false
      }
    },
    "capabilities": [
      "image-enhancement"
    ],
    "tags": [
      "image-enhancement"
    ],
    "lastUpdated": 1750566078555
  },
  "fal-ai/diffrhythm": {
    "id": "fal-ai/diffrhythm",
    "name": "DiffRhythm: Lyrics to Song",
    "category": "text-to-audio",
    "description": "DiffRhythm is a blazing fast model for transforming lyrics into full songs. It boasts the capability to generate full songs in less than 30 seconds.",
    "parameters": {
      "lyrics": {
        "type": "string",
        "description": "The prompt to generate the song from. Must have two sections. Sections start with either [chorus] or a [verse].",
        "required": true,
        "default": ""
      },
      "reference_audio_url": {
        "type": "string",
        "description": "The URL of the reference audio to use for the music generation.",
        "required": false,
        "default": ""
      },
      "style_prompt": {
        "type": "string",
        "description": "The style prompt to use for the music generation.",
        "required": false,
        "default": "",
        "enum": [
          "pop"
        ]
      },
      "music_duration": {
        "type": "string",
        "description": "The duration of the music to generate.",
        "required": false,
        "default": "95s",
        "enum": [
          "95s",
          "285s"
        ]
      },
      "cfg_strength": {
        "type": "number",
        "description": "The CFG strength to use for the music generation.",
        "required": false,
        "default": 4,
        "min": 1,
        "max": 10
      },
      "scheduler": {
        "type": "string",
        "description": "The scheduler to use for the music generation.",
        "required": false,
        "default": "euler",
        "enum": [
          "euler",
          "midpoint",
          "rk4",
          "implicit_adams"
        ]
      },
      "num_inference_steps": {
        "type": "integer",
        "description": "The number of inference steps to use for the music generation.",
        "required": false,
        "default": 32,
        "min": 10,
        "max": 100
      }
    },
    "capabilities": [
      "text-to-audio",
      "music generation"
    ],
    "tags": [
      "music"
    ],
    "lastUpdated": 1750566087147
  },
  "fal-ai/cogview4": {
    "id": "fal-ai/cogview4",
    "name": "CogView",
    "category": "text-to-image",
    "description": "Generate high quality images from text prompts using CogView4. Longer text prompts will result in better quality images.",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "The prompt to generate an image from.",
        "required": true
      },
      "image_size": {
        "type": "string",
        "description": "The size of the generated image. Default value: `landscape_4_3`",
        "required": false,
        "default": "landscape_4_3",
        "enum": [
          "square_hd",
          "square",
          "portrait_4_3",
          "portrait_16_9",
          "landscape_4_3",
          "landscape_16_9"
        ]
      },
      "negative_prompt": {
        "type": "string",
        "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution). Default value: `\"\"`",
        "required": false,
        "default": ""
      },
      "seed": {
        "type": "integer",
        "description": "The same seed and the same prompt given to the same version of the model will output the same image every time.",
        "required": false
      },
      "guidance_scale": {
        "type": "number",
        "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`",
        "required": false,
        "default": 3.5,
        "min": 0,
        "max": 20
      },
      "num_inference_steps": {
        "type": "integer",
        "description": "The number of inference steps to perform. Default value: `50`",
        "required": false,
        "default": 50,
        "min": 1,
        "max": 50
      },
      "num_images": {
        "type": "integer",
        "description": "The number of images to generate. Default value: `1`",
        "required": false,
        "default": 1,
        "min": 1,
        "max": 4
      },
      "sync_mode": {
        "type": "boolean",
        "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.",
        "required": false,
        "default": false
      },
      "enable_safety_checker": {
        "type": "boolean",
        "description": "If set to true, the safety checker will be enabled. Default value: `true`",
        "required": false,
        "default": true
      },
      "output_format": {
        "type": "string",
        "description": "The format of the generated image. Default value: `\"jpeg\"`",
        "required": false,
        "default": "jpeg",
        "enum": [
          "jpeg",
          "png"
        ]
      }
    },
    "capabilities": [
      "inference",
      "commercial use"
    ],
    "tags": [
      "stylized"
    ],
    "lastUpdated": 1750566088693
  },
  "fal-ai/ddcolor": {
    "id": "fal-ai/ddcolor",
    "name": "DDColor",
    "category": "image-to-image",
    "description": "Bring colors into old or new black and white photos with DDColor.",
    "parameters": {
      "image_url": {
        "type": "string",
        "description": "URL of image to be used for relighting",
        "required": true,
        "default": null
      },
      "seed": {
        "type": "integer",
        "description": "seed to be used for generation",
        "required": false,
        "default": null
      }
    },
    "capabilities": [
      "image-recolorization",
      "faces",
      "utility"
    ],
    "tags": [
      "image-recolorization",
      "faces",
      "utility"
    ],
    "lastUpdated": 1750566098996
  },
  "fal-ai/evf-sam": {
    "id": "fal-ai/evf-sam",
    "name": "EVF-SAM2 Segmentation",
    "category": "image-to-image",
    "description": "EVF-SAM2 combines natural language understanding with advanced segmentation capabilities, allowing you to precisely mask image regions using intuitive positive and negative text prompts.",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "The prompt to generate segmentation from.",
        "required": true,
        "default": null
      },
      "negative_prompt": {
        "type": "string",
        "description": "Areas to exclude from segmentation (will be subtracted from prompt results)",
        "required": false,
        "default": null
      },
      "semantic_type": {
        "type": "boolean",
        "description": "Enable semantic level segmentation for body parts, background or multi objects",
        "required": false,
        "default": false
      },
      "image_url": {
        "type": "string",
        "description": "URL of the input image",
        "required": true,
        "default": null
      },
      "mask_only": {
        "type": "boolean",
        "description": "Output only the binary mask instead of masked image",
        "required": false,
        "default": true
      },
      "use_grounding_dino": {
        "type": "boolean",
        "description": "Use GroundingDINO instead of SAM for segmentation",
        "required": false,
        "default": false
      },
      "revert_mask": {
        "type": "boolean",
        "description": "Invert the mask (background becomes foreground and vice versa)",
        "required": false,
        "default": false
      },
      "blur_mask": {
        "type": "integer",
        "description": "Apply Gaussian blur to the mask. Value determines kernel size (must be odd number)",
        "required": false,
        "default": 0,
        "min": 0,
        "max": 50
      },
      "expand_mask": {
        "type": "integer",
        "description": "Expand/dilate the mask by specified pixels",
        "required": false,
        "default": 0,
        "min": 0,
        "max": 20
      },
      "fill_holes": {
        "type": "boolean",
        "description": "Fill holes in the mask using morphological operations",
        "required": false,
        "default": false
      }
    },
    "capabilities": [
      "segmentation",
      "mask"
    ],
    "tags": [
      "segmentation",
      "mask"
    ],
    "lastUpdated": 1750566113165
  },
  "fal-ai/skyreels-i2v": {
    "id": "fal-ai/skyreels-i2v",
    "name": "Skyreels V1 (Image-to-Video)",
    "category": "image-to-video",
    "description": "SkyReels V1 is the first and most advanced open-source human-centric video foundation model. By fine-tuning HunyuanVideo on O(10M) high-quality film and television clips.",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "The prompt to generate the video from.",
        "required": true,
        "default": null
      },
      "image_url": {
        "type": "string",
        "description": "URL of the image input.",
        "required": true,
        "default": null
      },
      "seed": {
        "type": "integer",
        "description": "Random seed for generation. If not provided, a random seed will be used.",
        "required": false,
        "default": null
      },
      "guidance_scale": {
        "type": "number",
        "description": "Guidance scale for generation (between 1.0 and 20.0).",
        "required": false,
        "default": 6,
        "min": 1,
        "max": 20
      },
      "num_inference_steps": {
        "type": "integer",
        "description": "Number of denoising steps (between 1 and 50). Higher values give better quality but take longer.",
        "required": false,
        "default": 30,
        "min": 1,
        "max": 50
      },
      "negative_prompt": {
        "type": "string",
        "description": "Negative prompt to guide generation away from certain attributes.",
        "required": false,
        "default": null
      },
      "aspect_ratio": {
        "type": "string",
        "description": "Aspect ratio of the output video.",
        "required": false,
        "default": "16:9",
        "enum": [
          "16:9",
          "9:16"
        ]
      }
    },
    "capabilities": [
      "inference",
      "commercial use"
    ],
    "tags": [
      "motion"
    ],
    "lastUpdated": 1750566162267
  },
  "fal-ai/drct-super-resolution": {
    "id": "fal-ai/drct-super-resolution",
    "name": "DRCT-Super-Resolution",
    "category": "image-to-image",
    "description": "Upscale your images with DRCT-Super-Resolution.",
    "parameters": {
      "image_url": {
        "type": "string",
        "description": "URL of the image to upscale.",
        "required": true,
        "default": null
      },
      "upscaling_factor": {
        "type": "number",
        "description": "Upscaling factor. Default value: `4`",
        "required": false,
        "default": 4,
        "enum": [
          4
        ]
      }
    },
    "capabilities": [
      "inference"
    ],
    "tags": [
      "upscaling",
      "high-res"
    ],
    "lastUpdated": 1750566163275
  },
  "fal-ai/kokoro": {
    "id": "fal-ai/kokoro",
    "name": "Kokoro",
    "category": "unknown",
    "description": "Ready-to-use REST inference API, best in class performance, no coldstarts, fair pricing.",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "Default value: \"\"",
        "required": false,
        "default": ""
      },
      "voice": {
        "type": "enum",
        "description": "Voice ID for the desired voice. Default value: \"af_heart\"",
        "required": false,
        "default": "af_heart",
        "enum": [
          "af_heart",
          "af_alloy",
          "af_aoede",
          "af_bella",
          "af_jessica",
          "af_kore",
          "af_nicole",
          "af_nova",
          "af_river",
          "af_sarah",
          "af_sky",
          "am_adam",
          "am_echo",
          "am_eric",
          "am_fenrir",
          "am_liam",
          "am_michael",
          "am_onyx",
          "am_puck",
          "am_santa"
        ]
      },
      "speed": {
        "type": "number",
        "description": "Speed of the generated audio. Default is 1.0. Default value: 1",
        "required": false,
        "default": 1,
        "min": 0.1,
        "max": 5
      }
    },
    "capabilities": [
      "inference"
    ],
    "tags": [
      "private"
    ],
    "lastUpdated": 1750566174341
  },
  "fal-ai/fast-sdxl": {
    "id": "fal-ai/fast-sdxl",
    "name": "Stable Diffusion XL",
    "category": "text-to-image",
    "description": "Run SDXL at the speed of light",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "The prompt to use for generating the image. Be as descriptive as possible for best results.",
        "required": true
      },
      "negative_prompt": {
        "type": "string",
        "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution).",
        "required": false,
        "default": ""
      },
      "image_size": {
        "type": "string",
        "description": "The size of the generated image.",
        "required": false,
        "default": "square_hd",
        "enum": [
          "square_hd",
          "square",
          "portrait_4_3",
          "portrait_16_9",
          "landscape_4_3",
          "landscape_16_9"
        ]
      },
      "num_inference_steps": {
        "type": "integer",
        "description": "The number of inference steps to perform.",
        "required": false,
        "default": 25,
        "min": 1,
        "max": 50
      },
      "guidance_scale": {
        "type": "float",
        "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.",
        "required": false,
        "default": 7.5,
        "min": 0,
        "max": 20
      },
      "num_images": {
        "type": "integer",
        "description": "The number of images to generate.",
        "required": false,
        "default": 1,
        "min": 1,
        "max": 8
      },
      "loras": {
        "type": "array",
        "description": "The list of LoRA weights to use.",
        "required": false,
        "default": []
      },
      "embeddings": {
        "type": "array",
        "description": "The list of embeddings to use.",
        "required": false,
        "default": []
      },
      "enable_safety_checker": {
        "type": "boolean",
        "description": "If set to true, the safety checker will be enabled.",
        "required": false,
        "default": true
      },
      "safety_checker_version": {
        "type": "string",
        "description": "The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model.",
        "required": false,
        "default": "v1",
        "enum": [
          "v1",
          "v2"
        ]
      },
      "format": {
        "type": "string",
        "description": "The format of the generated image.",
        "required": false,
        "default": "jpeg",
        "enum": [
          "jpeg",
          "png"
        ]
      },
      "request_id": {
        "type": "string",
        "description": "An id bound to a request, can be used with response to identify the request itself.",
        "required": false,
        "default": ""
      }
    },
    "capabilities": [
      "text-to-image"
    ],
    "tags": [
      "diffusion",
      "lora",
      "embeddings",
      "high-res",
      "style"
    ],
    "lastUpdated": 1750566816476
  },
  "fal-ai/flux/schnell": {
    "id": "fal-ai/flux/schnell",
    "name": "FLUX.1 [schnell]",
    "category": "text-to-image",
    "description": "FLUX.1 [schnell] is a 12 billion parameter flow transformer that generates high-quality images from text in 1 to 4 steps, suitable for personal and commercial use.",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "The prompt to generate an image from.",
        "required": true
      },
      "image_size": {
        "type": "string",
        "description": "The size of the generated image.",
        "required": false,
        "default": "landscape_4_3",
        "enum": [
          "square_hd",
          "square",
          "portrait_4_3",
          "portrait_16_9",
          "landscape_4_3",
          "landscape_16_9"
        ]
      },
      "num_inference_steps": {
        "type": "number",
        "description": "The number of inference steps to perform.",
        "required": false,
        "default": 4,
        "min": 1,
        "max": 12
      },
      "num_images": {
        "type": "number",
        "description": "The number of images to generate.",
        "required": false,
        "default": 1,
        "min": 1,
        "max": 4
      },
      "enable_safety_checker": {
        "type": "boolean",
        "description": "If set to true, the safety checker will be enabled.",
        "required": false,
        "default": true
      }
    },
    "capabilities": [
      "text-to-image",
      "high-quality image generation"
    ],
    "tags": [
      "commercial use",
      "fast inference"
    ],
    "lastUpdated": 1750617163333
  },
  "fal-ai/svd": {
    "id": "fal-ai/svd",
    "name": "Svd",
    "category": "unknown",
    "description": "Generate Video",
    "parameters": {
      "image_url": {
        "type": "string",
        "description": "The URL of the image to use as a starting point for the generation.",
        "required": true
      },
      "mask_image_url": {
        "type": "string",
        "description": "The URL of the image to use as a mask for the generation. The mask should be a black and white image, where black represents the area that will move, and white represents the area that will stay still.",
        "required": false
      },
      "control_maps_data_url": {
        "type": "string",
        "description": "A sequence of depths maps as a archive file url. A depth map is need for every frame. The frame count for the video will equal the number of depth map frames in the archive. The images will be sorted by file name. Acceptable image extensions are: '.jpg', '.jpeg', '.png', '.bmp', '.gif', or '.tiff'. All other files will be ignored.",
        "required": false
      },
      "motion_bucket_id": {
        "type": "integer",
        "description": "The motion bucket id determines the motion of the generated video. The higher the number, the more motion there will be.",
        "required": false,
        "default": 127,
        "min": 1,
        "max": 255
      },
      "cond_aug": {
        "type": "float",
        "description": "The conditoning augmentation determines the amount of noise that will be added to the conditioning frame. The higher the number, the more noise there will be, and the less the video will look like the initial image. Increase it for more motion.",
        "required": false,
        "default": 0.02,
        "min": 0,
        "max": 10
      },
      "seed": {
        "type": "integer",
        "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time.",
        "required": false
      },
      "steps": {
        "type": "integer",
        "description": "The number of steps to run the model for. The higher the number the better the quality and longer it will take to generate.",
        "required": false,
        "default": 20,
        "min": 1,
        "max": 100
      }
    },
    "capabilities": [
      "inference"
    ],
    "tags": [],
    "lastUpdated": 1750619368864
  }
}